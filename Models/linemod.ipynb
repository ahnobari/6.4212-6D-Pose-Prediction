{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543fa26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "import trimesh\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "import random\n",
    "\n",
    "import roma\n",
    "\n",
    "# from epd import *\n",
    "from tqdm.autonotebook import tqdm\n",
    "import scipy\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41b201e",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf63aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('processed_data_linemod.h5', 'r')\n",
    "rgbs = hf['rgbs'][:]\n",
    "depth_imgs = hf['depth_imgs'][:]\n",
    "merged_masks = hf['merged_masks'][:]\n",
    "point_cloud_pairs = np.load('point_cloud_pairs_linemod.npy',allow_pickle=True)\n",
    "gt = np.load('linemod_gt.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493556c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_mask_value = {\"ape\": 21,\n",
    "                       \"can\": 106,\n",
    "                       \"cat\": 128,\n",
    "                       \"driller\": 170,\n",
    "                       \"duck\": 191,\n",
    "                       \"eggbox\": 213,\n",
    "                       \"glue\": 234,\n",
    "                       \"holepuncher\": 255}\n",
    "class_to_name = {0: \"ape\", 1: \"can\", 2: \"cat\", 3: \"driller\", 4: \"duck\", 5: \"eggbox\", 6: \"glue\", 7: \"holepuncher\"}\n",
    "name_to_id = {\"ape\": 1, \"can\": 5, \"cat\": 6, \"driller\": 8, \"duck\": 9, \"eggbox\": 10, \"glue\": 11, \"holepuncher\": 12}\n",
    "id_to_name = {v: k for k, v in name_to_id.items()}\n",
    "name_to_class = {v: k for k, v in class_to_name.items()}\n",
    "obj_mask_value = {21: 1, 106: 5, 128: 6, 170: 8, 191: 9, 213: 10, 234: 11, 255: 12}\n",
    "obj_to_mask_value = {v: k for k, v in obj_mask_value.items()}\n",
    "\n",
    "reference_point_clouds = np.load('reference_point_clouds_linemod.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0b14b4",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f406a5fb",
   "metadata": {},
   "source": [
    "### Reference Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de10a1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.Scene()\n",
    "for i,pc in enumerate(reference_point_clouds):\n",
    "    scene.add_geometry(trimesh.points.PointCloud(pc[0] + [i%6 * 300, i//6 * 300 ,0],colors=pc[1]))\n",
    "scene.camera.z_far = 10\n",
    "scene.show(viewer='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba06da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "diameters = []\n",
    "for i in range(len(reference_point_clouds)):\n",
    "    diameters.append(float(torch.cdist(torch.Tensor(reference_point_clouds[i][0]),torch.Tensor(reference_point_clouds[i][0])).max().detach().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61ed444",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1ae8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_pair = point_cloud_pairs[np.random.choice(point_cloud_pairs.shape[0])]\n",
    "rgb = rgbs[point_cloud_pair[-1]]\n",
    "merged_mask = merged_masks[point_cloud_pair[-1]]\n",
    "depth_img = depth_imgs[point_cloud_pair[-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be131308",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(merged_mask,cmap='Greys_r')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0c35bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.Scene()\n",
    "scene.add_geometry(trimesh.points.PointCloud(depth_img[:,:,0:3].reshape([-1,3]),colors=rgb.reshape([-1,3])))\n",
    "scene.camera.z_far = 10\n",
    "scene.show(viewer='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191bfd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.Scene()\n",
    "obj_id_list = list(id_to_name.keys())\n",
    "\n",
    "pc_source = trimesh.points.PointCloud(point_cloud_pair[0],colors=point_cloud_pair[1])\n",
    "scene.add_geometry(pc_source)\n",
    "\n",
    "r_pc = np.copy(reference_point_clouds[obj_id_list.index(point_cloud_pair[2])][0])\n",
    "r_pc = (np.reshape(point_cloud_pair[3],(3,3)) @ r_pc.T).T + point_cloud_pair[4]\n",
    "pc_target = trimesh.points.PointCloud(r_pc,colors=[0,255,0])\n",
    "scene.add_geometry(pc_target)\n",
    "\n",
    "scene.camera.z_far = 10\n",
    "scene.show(viewer='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997edea9",
   "metadata": {},
   "source": [
    "## Instance Segmentaion Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9795e0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManipulationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(range(len(rgbs)))\n",
    "        self.masks = list(range(len(rgbs)))\n",
    "        \n",
    "        self.obj_id_dict = {0: \"ape\", 1: \"can\", 2: \"cat\", 3: \"driller\", 4: \"duck\", 5: \"eggbox\", 6: \"glue\", 7: \"holepuncher\"}\n",
    "        self.obj_mask_value = {21: 0, 106: 1, 128: 2, 170: 3, 191: 4, 213: 5, 234: 6, 255: 7}\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        # img_path = os.path.join(self.root, \"rgb\", self.imgs[idx])\n",
    "        # mask_path = os.path.join(self.root, \"merged_masks\", self.masks[idx])\n",
    "        img = torch.Tensor(np.transpose(rgbs[self.imgs[idx]],[2,0,1]))\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = merged_masks[self.masks[idx]]\n",
    "\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        \n",
    "        labels = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            labels[i] = self.obj_mask_value[obj_ids[i]] + 1\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        img = img.to(torch.float32)/255.0\n",
    "        \n",
    "        # Normalizer = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60140c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Stop here if you are fine-tunning Faster-RCNN\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90a13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 59, 60, 61, 62, 64, 66, 67, 68, 69, 70, 73, 74, 75, 77, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 93, 94, 96, 97, 98, 100, 101, 102, 104, 105, 107, 108, 109, 110, 111, 112, 113, 114, 115, 117, 118, 119, 120, 121, 122, 124, 125, 126, 127, 128, 129, 131, 132, 133, 135, 136, 137, 138, 140, 141, 142, 143, 144, 145, 147, 148, 149, 150, 151, 154, 156, 159, 160, 162, 164, 165, 166, 168, 169, 170, 171, 173, 175, 176, 177, 178, 179, 180, 181, 182, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 215, 216, 217, 218, 219, 220, 222, 223, 224, 225, 227, 228, 229, 230, 231, 232, 233, 234, 236, 237, 238, 240, 241, 242, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 272, 273, 275, 276, 278, 279, 280, 281, 282, 283, 284, 285, 287, 288, 289, 290, 292, 293, 295, 296, 297, 298, 299, 300, 301, 303, 304, 305, 306, 308, 309, 310, 311, 312, 313, 315, 316, 317, 318, 319, 321, 322, 323, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 348, 349, 351, 352, 353, 354, 356, 357, 358, 359, 360, 361, 362, 363, 365, 366, 368, 370, 371, 372, 373, 374, 375, 378, 380, 381, 382, 385, 386, 388, 389, 390, 391, 392, 393, 395, 396, 397, 398, 399, 400, 401, 403, 404, 405, 407, 408, 409, 411, 412, 414, 415, 416, 417, 418, 419, 420, 421, 423, 424, 426, 427, 428, 429, 431, 432, 433, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 447, 448, 449, 450, 452, 453, 454, 455, 457, 458, 459, 460, 462, 463, 464, 466, 467, 468, 469, 470, 472, 473, 474, 475, 476, 477, 478, 479, 481, 482, 484, 485, 486, 487, 488, 489, 490, 491, 492, 494, 495, 497, 499, 500, 501, 502, 503, 504, 505, 506, 508, 509, 510, 511, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 526, 528, 529, 530, 531, 535, 536, 537, 538, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 555, 557, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 569, 570, 572, 574, 575, 577, 578, 579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599, 600, 601, 602, 605, 606, 607, 608, 610, 611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623, 624, 625, 626, 628, 629, 630, 631, 632, 633, 634, 636, 637, 638, 639, 640, 642, 643, 644, 645, 646, 647, 648, 650, 651, 652, 654, 655, 657, 658, 660, 661, 662, 663, 664, 665, 666, 667, 669, 670, 671, 672, 673, 674, 675, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689, 690, 691, 693, 694, 695, 696, 698, 699, 700, 701, 702, 703, 704, 705, 707, 708, 709, 710, 711, 712, 713, 714, 716, 718, 719, 720, 721, 722, 723, 724, 725, 727, 728, 729, 730, 731, 732, 733, 734, 736, 737, 738, 739, 740, 741, 742, 743, 745, 746, 748, 749, 750, 751, 753, 754, 755, 756, 757, 759, 761, 762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 773, 774, 776, 777, 778, 779, 781, 782, 783, 784, 786, 787, 788, 789, 790, 791, 792, 793, 794, 795, 796, 797, 798, 799, 801, 803, 804, 805, 806, 807, 808, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 820, 821, 822, 823, 824, 825, 826, 827, 829, 830, 831, 832, 833, 834, 835, 836, 838, 839, 840, 841, 843, 844, 846, 848, 849, 851, 852, 853, 854, 855, 856, 857, 858, 860, 861, 862, 863, 864, 865, 866, 867, 868, 869, 870, 871, 872, 873, 874, 876, 877, 878, 879, 881, 882, 884, 885, 886, 887, 888, 889, 890, 893, 894, 895, 896, 897, 898, 899, 900, 901, 902, 903, 904, 905, 906, 907, 908, 909, 910, 911, 912, 913, 914, 917, 918, 919, 920, 921, 922, 924, 925, 926, 927, 928, 929, 930, 932, 934, 935, 936, 937, 938, 939, 940, 942, 943, 944, 946, 947, 948, 949, 950, 951, 952, 953, 955, 956, 957, 958, 960, 961, 962, 963, 965, 966, 967, 968, 969, 970, 971, 972, 973, 974, 976, 977, 978, 979, 980, 981, 982, 983, 984, 985, 986, 988, 989, 990, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1003, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1015, 1016, 1017, 1018, 1019, 1021, 1022, 1023, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1039, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1049, 1050, 1051, 1052, 1053, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1062, 1063, 1064, 1065, 1067, 1068, 1069, 1070, 1072, 1073, 1074, 1075, 1076, 1077, 1078, 1079, 1080, 1082, 1083, 1085, 1086, 1087, 1089, 1090, 1091, 1092, 1093, 1094, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1104, 1105, 1107, 1108, 1109, 1110, 1111, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1122, 1123, 1124, 1125, 1126, 1127, 1128, 1130, 1131, 1132, 1134, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1158, 1160, 1161, 1162, 1164, 1165, 1166, 1167, 1168, 1169, 1170, 1173, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1185, 1186, 1187, 1188, 1189, 1190, 1192, 1193, 1194, 1195, 1196, 1197, 1199, 1200, 1201, 1202, 1203, 1204, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213]\n",
    "test = [24, 30, 45, 53, 63, 65, 71, 72, 76, 78, 91, 92, 95, 99, 103, 106, 116, 123, 130, 134, 139, 146, 152, 153, 155, 157, 158, 161, 163, 167, 172, 174, 183, 200, 214, 221, 226, 235, 239, 243, 271, 274, 277, 286, 291, 294, 302, 307, 314, 320, 324, 347, 350, 355, 364, 367, 369, 376, 377, 379, 383, 384, 387, 394, 402, 406, 410, 413, 422, 425, 430, 434, 441, 446, 451, 456, 461, 465, 471, 480, 483, 493, 496, 498, 507, 512, 525, 527, 532, 533, 534, 539, 554, 556, 568, 571, 573, 576, 598, 603, 604, 609, 627, 635, 641, 649, 653, 656, 659, 668, 676, 692, 697, 706, 715, 717, 726, 735, 744, 747, 752, 758, 760, 772, 775, 780, 785, 800, 802, 828, 837, 842, 845, 847, 850, 859, 875, 880, 883, 915, 916, 923, 931, 933, 941, 945, 954, 959, 964, 975, 987, 1002, 1014, 1020, 1024, 1038, 1040, 1048, 1066, 1071, 1081, 1084, 1088, 1102, 1103, 1106, 1112, 1121, 1129, 1133, 1135, 1136, 1157, 1159, 1163, 1171, 1172, 1174, 1191, 1198, 1205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f5922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "dataset = ManipulationDataset('../manipulation/dataset/', get_transform(train=True))\n",
    "dataset_test = ManipulationDataset('../manipulation/dataset/', get_transform(train=False))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dataset = torch.utils.data.Subset(dataset, train)\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=16, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=16, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccf1eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 9\n",
    "\n",
    "model = build_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=30,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b1e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "if os.path.exists('mask-rcnn-linemod.pt'):\n",
    "    model = torch.load('mask-rcnn-linemod.pt')\n",
    "else:\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100000, epoch_length=int(split/16))\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48de8af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "CLASS_NAMES = ['__background__', \n",
    "               \"ape\",\n",
    "               \"can\",\n",
    "               \"cat\",\n",
    "               \"driller\",\n",
    "               \"duck\",\n",
    "               \"eggbox\",\n",
    "               \"glue\",\n",
    "               \"holepuncher\"]\n",
    "device = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e97872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coloured_mask(mask):\n",
    "    \"\"\"\n",
    "    random_colour_masks\n",
    "      parameters:\n",
    "        - image - predicted masks\n",
    "      method:\n",
    "        - the masks of each predicted object is given random colour for visualization\n",
    "    \"\"\"\n",
    "    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
    "    r = np.zeros_like(mask).astype(np.uint8)\n",
    "    g = np.zeros_like(mask).astype(np.uint8)\n",
    "    b = np.zeros_like(mask).astype(np.uint8)\n",
    "    r[mask == 1], g[mask == 1], b[mask == 1] = colours[random.randrange(0,10)]\n",
    "    coloured_mask = np.stack([r, g, b], axis=2)\n",
    "    return coloured_mask\n",
    "\n",
    "def get_prediction(img, confidence):\n",
    "    \"\"\"\n",
    "    get_prediction\n",
    "      parameters:\n",
    "        - img_path - path of the input image\n",
    "        - confidence - threshold to keep the prediction or not\n",
    "      method:\n",
    "        - Image is obtained from the image path\n",
    "        - the image is converted to image tensor using PyTorch's Transforms\n",
    "        - image is passed through the model to get the predictions\n",
    "        - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks\n",
    "          ie: eg. segment of cat is made 1 and rest of the image is made 0\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    img = img.to(device)\n",
    "    pred = model([img])\n",
    "    pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x>confidence][-1]\n",
    "    masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
    "    pred_class = [CLASS_NAMES[i] for i in list(pred[0]['labels'].cpu().numpy())]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]\n",
    "    masks = masks[:pred_t+1]\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    return masks, pred_boxes, pred_class\n",
    "\n",
    "def segment_instance(img, rgb_img, confidence=0.5, rect_th=2, text_size=0.75, text_th=2):\n",
    "    \"\"\"\n",
    "    segment_instance\n",
    "      parameters:\n",
    "        - img_path - path to input image\n",
    "        - confidence- confidence to keep the prediction or not\n",
    "        - rect_th - rect thickness\n",
    "        - text_size\n",
    "        - text_th - text thickness\n",
    "      method:\n",
    "        - prediction is obtained by get_prediction\n",
    "        - each mask is given random color\n",
    "        - each mask is added to the image in the ration 1:0.8 with opencv\n",
    "        - final output is displayed\n",
    "    \"\"\"\n",
    "    masks, boxes, pred_cls = get_prediction(img, confidence)\n",
    "    img = rgb_img\n",
    "    for i in range(len(masks)):\n",
    "        rgb_mask = get_coloured_mask(masks[i])\n",
    "        img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "        cv2.rectangle(img, [int(boxes[i][0][0]),int(boxes[i][0][1])],[int(boxes[i][1][0]),int(boxes[i][1][1])],color=(0, 255, 0), thickness=rect_th)\n",
    "        cv2.putText(img,pred_cls[i], [int(boxes[i][0][0]),int(boxes[i][0][1])], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297dd1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_i = np.random.choice(len(test))\n",
    "segment_instance(dataset_test[rand_i][0],rgbs[test[rand_i]],confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaae606b",
   "metadata": {},
   "source": [
    "## Pose Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68de26a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "class TransformerEncoderLayer_NoNorm(nn.Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of the intermediate layer, can be a string\n",
    "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n",
    "            operations, respectively. Otherwise it's done after. Default: ``False`` (after).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Fast path:\n",
    "        forward() will use a special optimized implementation if all of the following\n",
    "        conditions are met:\n",
    "\n",
    "        - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor\n",
    "          argument ``requires_grad``\n",
    "        - training is disabled (using ``.eval()``)\n",
    "        - batch_first is ``True`` and the input is batched (i.e., ``src.dim() == 3``)\n",
    "        - activation is one of: ``\"relu\"``, ``\"gelu\"``, ``torch.functional.relu``, or ``torch.functional.gelu``\n",
    "        - at most one of ``src_mask`` and ``src_key_padding_mask`` is passed\n",
    "        - if src is a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_, neither ``src_mask``\n",
    "          nor ``src_key_padding_mask`` is passed\n",
    "        - the two ``LayerNorm`` instances have a consistent ``eps`` value (this will naturally be the case\n",
    "          unless the caller has manually modified one without modifying the other)\n",
    "\n",
    "        If the optimized implementation is in use, a\n",
    "        `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be\n",
    "        passed for ``src`` to represent padding more efficiently than using a padding\n",
    "        mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ will be\n",
    "        returned, and an additional speedup proportional to the fraction of the input that\n",
    "        is padding can be expected.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[torch.Tensor], torch.Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer_NoNorm, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "#         self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "#         self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            activation = _get_activation_fn(activation)\n",
    "\n",
    "        # We can't test self.activation in forward() in TorchScript,\n",
    "        # so stash some information about it instead.\n",
    "        if activation is F.relu or isinstance(activation, torch.nn.ReLU):\n",
    "            self.activation_relu_or_gelu = 1\n",
    "        elif activation is F.gelu or isinstance(activation, torch.nn.GELU):\n",
    "            self.activation_relu_or_gelu = 2\n",
    "        else:\n",
    "            self.activation_relu_or_gelu = 0\n",
    "        self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(TransformerEncoderLayer_NoNorm, self).__setstate__(state)\n",
    "        if not hasattr(self, 'activation'):\n",
    "            self.activation = F.relu\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None,\n",
    "                src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "\n",
    "        if src_key_padding_mask is not None:\n",
    "            _skpm_dtype = src_key_padding_mask.dtype\n",
    "            if _skpm_dtype != torch.bool and not torch.is_floating_point(src_key_padding_mask):\n",
    "                raise AssertionError(\n",
    "                    \"only bool and floating types of key_padding_mask are supported\")\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "        why_not_sparsity_fast_path = ''\n",
    "        if not src.dim() == 3:\n",
    "            why_not_sparsity_fast_path = f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n",
    "        elif self.training:\n",
    "            why_not_sparsity_fast_path = \"training is enabled\"\n",
    "        elif not self.self_attn.batch_first :\n",
    "            why_not_sparsity_fast_path = \"self_attn.batch_first was not True\"\n",
    "        elif not self.self_attn._qkv_same_embed_dim :\n",
    "            why_not_sparsity_fast_path = \"self_attn._qkv_same_embed_dim was not True\"\n",
    "        elif not self.activation_relu_or_gelu:\n",
    "            why_not_sparsity_fast_path = \"activation_relu_or_gelu was not True\"\n",
    "        elif not (self.norm1.eps == self.norm2.eps):\n",
    "            why_not_sparsity_fast_path = \"norm1.eps is not equal to norm2.eps\"\n",
    "        elif src_mask is not None:\n",
    "            why_not_sparsity_fast_path = \"src_mask is not supported for fastpath\"\n",
    "        elif src.is_nested and src_key_padding_mask is not None:\n",
    "            why_not_sparsity_fast_path = \"src_key_padding_mask is not supported with NestedTensor input for fastpath\"\n",
    "        elif self.self_attn.num_heads % 2 == 1:\n",
    "            why_not_sparsity_fast_path = \"num_head is odd\"\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_sparsity_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_sparsity_fast_path:\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                self.self_attn.in_proj_weight,\n",
    "                self.self_attn.in_proj_bias,\n",
    "                self.self_attn.out_proj.weight,\n",
    "                self.self_attn.out_proj.bias,\n",
    "#                 self.norm1.weight,\n",
    "#                 self.norm1.bias,\n",
    "#                 self.norm2.weight,\n",
    "#                 self.norm2.bias,\n",
    "                self.linear1.weight,\n",
    "                self.linear1.bias,\n",
    "                self.linear2.weight,\n",
    "                self.linear2.bias,\n",
    "            )\n",
    "\n",
    "            # We have to use list comprehensions below because TorchScript does not support\n",
    "            # generator expressions.\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif not all((x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args):\n",
    "                why_not_sparsity_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\n",
    "            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n",
    "                why_not_sparsity_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                              \"input/output projection weights or biases requires_grad\")\n",
    "\n",
    "            if not why_not_sparsity_fast_path:\n",
    "                return torch._transformer_encoder_layer_fwd(\n",
    "                    src,\n",
    "                    self.self_attn.embed_dim,\n",
    "                    self.self_attn.num_heads,\n",
    "                    self.self_attn.in_proj_weight,\n",
    "                    self.self_attn.in_proj_bias,\n",
    "                    self.self_attn.out_proj.weight,\n",
    "                    self.self_attn.out_proj.bias,\n",
    "                    self.activation_relu_or_gelu == 2,\n",
    "                    self.norm_first,\n",
    "                    self.norm1.eps,\n",
    "                    self.norm1.weight,\n",
    "                    self.norm1.bias,\n",
    "                    self.norm2.weight,\n",
    "                    self.norm2.bias,\n",
    "                    self.linear1.weight,\n",
    "                    self.linear1.bias,\n",
    "                    self.linear2.weight,\n",
    "                    self.linear2.bias,\n",
    "                    # TODO: if src_mask and src_key_padding_mask merge to single 4-dim mask\n",
    "                    src_mask if src_mask is not None else src_key_padding_mask,\n",
    "                    1 if src_key_padding_mask is not None else\n",
    "                    0 if src_mask is not None else\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(x, src_mask, src_key_padding_mask)\n",
    "            x = x + self._ff_block(x)\n",
    "        else:\n",
    "            x = x + self._sa_block(x, src_mask, src_key_padding_mask)\n",
    "            x = x + self._ff_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: torch.Tensor,\n",
    "                  attn_mask: Optional[torch.Tensor], key_padding_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c702476",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identical to DenseFusion\n",
    "from pspnet import PSPNet\n",
    "#Identical to DenseFusion\n",
    "psp_models = {\n",
    "    'resnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "    'resnet101': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet101'),\n",
    "    'resnet152': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet152')\n",
    "}\n",
    "#Identical to DenseFusion\n",
    "\n",
    "# New\n",
    "class ModifiedResnet(nn.Module):\n",
    "\n",
    "    def __init__(self, usegpu=True, large=False):\n",
    "        super(ModifiedResnet, self).__init__()\n",
    "        if large:\n",
    "            self.model = psp_models['resnet50'.lower()]()\n",
    "        else:\n",
    "            self.model = psp_models['resnet18'.lower()]()\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class TransformationNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TransformationNet, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.in1 = nn.Linear(input_dim, 64)\n",
    "        self.bn_1 = nn.LayerNorm(64)\n",
    "        \n",
    "        self.in2 = nn.Linear(64, 128)\n",
    "        self.bn_2 = nn.LayerNorm(128)\n",
    "        \n",
    "        self.in3 = nn.Linear(128, 128)\n",
    "        self.bn_3 = nn.LayerNorm(128)\n",
    "        \n",
    "        self.in4 = nn.Linear(128, 256)\n",
    "        self.bn_4 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.in5 = nn.Linear(256, 512)\n",
    "        self.bn_5 = nn.LayerNorm(512)\n",
    "\n",
    "        self.fc_1 = nn.Linear(512, 512)\n",
    "        self.bn_6 = nn.LayerNorm(512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.bn_7 = nn.LayerNorm(512)\n",
    "        self.fc_3 = nn.Linear(512, self.output_dim*self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_points = x.shape[1]\n",
    "        # x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_1(self.in1(x)))\n",
    "        x = F.relu(self.bn_2(self.in2(x)))\n",
    "        x = F.relu(self.bn_3(self.in3(x)))\n",
    "        x = F.relu(self.bn_4(self.in4(x)))\n",
    "        x = F.relu(self.bn_5(self.in5(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "        \n",
    "        x = nn.MaxPool1d(num_points)(x)\n",
    "        x = x.view(-1, 512)\n",
    "\n",
    "        x = F.relu(self.bn_6(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_7(self.fc_2(x)))\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        identity_matrix = torch.eye(self.output_dim)\n",
    "        if torch.cuda.is_available():\n",
    "            identity_matrix = identity_matrix.cuda()\n",
    "        x = x.view(-1, self.output_dim, self.output_dim) + identity_matrix\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasePointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, point_dimension):\n",
    "        super(BasePointNet, self).__init__()\n",
    "        self.input_transform = TransformationNet(input_dim=point_dimension, output_dim=point_dimension)\n",
    "        self.feature_transform = TransformationNet(input_dim=64, output_dim=64)\n",
    "        \n",
    "        self.conv_1 = nn.Linear(point_dimension, 64)\n",
    "        self.conv_2 = nn.Linear(64, 64)\n",
    "        self.conv_3 = nn.Linear(64, 128)\n",
    "        # self.conv_4 = nn.Linear(128, 128)\n",
    "        self.conv_5 = nn.Linear(128, 256)\n",
    "        # self.conv_6 = nn.Linear(256, 256)\n",
    "        self.conv_7 = nn.Linear(256, 1024)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(64)\n",
    "        self.bn_2 = nn.LayerNorm(64)\n",
    "        self.bn_3 = nn.LayerNorm(128)\n",
    "        # self.bn_4 = nn.LayerNorm(128)\n",
    "        self.bn_5 = nn.LayerNorm(256)\n",
    "        # self.bn_6 = nn.LayerNorm(256)\n",
    "        self.bn_7 = nn.LayerNorm(1024)\n",
    "        \n",
    "\n",
    "    def forward(self, x, plot=False):\n",
    "        num_points = x.shape[1]\n",
    "        \n",
    "        # input_transform = self.input_transform(x) # T-Net tensor [batch, 3, 3]\n",
    "        # x = torch.bmm(x, input_transform) # Batch matrix-matrix product \n",
    "        # x = x.transpose(2, 1)\n",
    "        tnet_out=x.cpu().detach().numpy()\n",
    "        feature_transform = torch.zeros([1,2,2]).to(device)\n",
    "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
    "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
    "        # x = x.transpose(2, 1)\n",
    "\n",
    "        # feature_transform = self.feature_transform(x) # T-Net tensor [batch, 64, 64]\n",
    "        # x = torch.bmm(x, feature_transform)\n",
    "        x_perp = x\n",
    "        # x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
    "        # x = F.relu(self.bn_4(self.conv_4(x)))\n",
    "        x = F.relu(self.bn_5(self.conv_5(x)))\n",
    "        # x = F.relu(self.bn_6(self.conv_6(x)))\n",
    "        x = F.relu(self.bn_7(self.conv_7(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "        x_g, ix = nn.MaxPool1d(num_points, return_indices=True)(x)  # max-pooling\n",
    "        x_g = x_g.view(-1, 1024)  # global feature vector \n",
    "        \n",
    "        return x_g, x_perp, feature_transform, tnet_out, ix\n",
    "\n",
    "class SegmentaionPointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, out_dim=128, dropout=0.3, point_dimension=6):\n",
    "        super(SegmentaionPointNet, self).__init__()\n",
    "        self.base_pointnet = BasePointNet(point_dimension=point_dimension)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(1088, 512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.fc_3 = nn.Linear(512, out_dim)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(512)\n",
    "        self.bn_2 = nn.LayerNorm(512)\n",
    "\n",
    "        # self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_g, x, feature_transform, tnet_out, ix_maxpool = self.base_pointnet(x)\n",
    "        x_g = torch.unsqueeze(x_g,1)\n",
    "        x_g = torch.tile(x_g,[1,x.shape[1],1])\n",
    "        x = torch.cat([x,x_g],-1)\n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_2(self.fc_2(x)))\n",
    "        # x = self.dropout_1(x)\n",
    "\n",
    "        return self.fc_3(x), feature_transform, tnet_out, ix_maxpool\n",
    "    \n",
    "class ClassificationPointNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_dim, dropout=0.3, point_dimension=6):\n",
    "        super(ClassificationPointNet, self).__init__()\n",
    "        self.base_pointnet = BasePointNet(point_dimension=point_dimension)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(1024, 512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.fc_3 = nn.Linear(512, out_dim)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(512)\n",
    "        self.bn_2 = nn.LayerNorm(512)\n",
    "\n",
    "        # self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _, feature_transform, tnet_out, ix_maxpool = self.base_pointnet(x)\n",
    "\n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_2(self.fc_2(x)))\n",
    "        # x = self.dropout_1(x)\n",
    "\n",
    "        return self.fc_3(x), feature_transform, tnet_out, ix_maxpool\n",
    "    \n",
    "class PointCloud_Matching(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PointCloud_Matching, self).__init__()\n",
    "        self.source_point_net = ClassificationPointNet(512)\n",
    "        self.target_point_net = ClassificationPointNet(512)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(1024, 512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.fc_3 = nn.Linear(512, 3)\n",
    "        self.fc_4 = nn.Linear(512, 3)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(512)\n",
    "        self.bn_2 = nn.LayerNorm(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        source_pcs, target_pcs = x\n",
    "        \n",
    "        s_x, s_ft, s_tn, _ = self.source_point_net(source_pcs)\n",
    "        t_x, t_ft, t_tn, _ = self.target_point_net(target_pcs)\n",
    "        \n",
    "        x = torch.cat([s_x,t_x],dim=-1)\n",
    "        \n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_2(self.fc_2(x)))\n",
    "        \n",
    "        R = torch.tanh(self.fc_3(x)) * np.pi\n",
    "        t = self.fc_4(x)\n",
    "        \n",
    "        return R, t, s_ft, t_ft\n",
    "    \n",
    "class PointCloud_Matching_pixel_wise(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PointCloud_Matching_pixel_wise, self).__init__()\n",
    "        self.source_point_net = SegmentaionPointNet(128,point_dimension=6)\n",
    "        self.target_point_net = ClassificationPointNet(64,point_dimension=6)\n",
    "        self.conv = ModifiedResnet()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(128+64, 32)\n",
    "        self.fc_2 = nn.Linear(64, 256)\n",
    "        self.fc_3 = nn.Linear(256, 32)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(32)\n",
    "        self.bn_2 = nn.LayerNorm(256)\n",
    "        self.bn_3 = nn.LayerNorm(32)\n",
    "        \n",
    "        self.r_fc_1 = nn.Linear(3*32, 512)\n",
    "        self.r_bn_1 = nn.LayerNorm(512)\n",
    "        self.r_fc_2 = nn.Linear(512, 512)\n",
    "        self.r_bn_2 = nn.LayerNorm(512)\n",
    "        self.r_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        \n",
    "        self.t_fc_1 = nn.Linear(3*32, 512)\n",
    "        self.t_bn_1 = nn.LayerNorm(512)\n",
    "        self.t_fc_2 = nn.Linear(512, 512)\n",
    "        self.t_bn_2 = nn.LayerNorm(512)\n",
    "        self.t_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        self.c_fc_1 = nn.Linear(3*32, 512)\n",
    "        self.c_bn_1 = nn.LayerNorm(512)\n",
    "        self.c_fc_2 = nn.Linear(512, 512)\n",
    "        self.c_bn_2 = nn.LayerNorm(512)\n",
    "        self.c_fc_3 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        source_pcs, target_pcs, rgb, mask = x\n",
    "        \n",
    "        per_pixel = self.conv(rgb)\n",
    "        per_pixel = torch.unsqueeze(torch.permute(per_pixel,[0,2,3,1])[0][mask],0)\n",
    "        \n",
    "        s_x, s_ft, s_tn, _ = self.source_point_net(source_pcs)\n",
    "        t_x, t_ft, t_tn, _ = self.target_point_net(target_pcs)\n",
    "        \n",
    "        t_x = torch.unsqueeze(t_x,1)\n",
    "        t_x = torch.tile(t_x,[1,s_x.shape[1],1])\n",
    "        \n",
    "        x = torch.cat([s_x,t_x],dim=-1)\n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        \n",
    "        x_perp = torch.cat([x,per_pixel],-1)\n",
    "        \n",
    "        x = F.relu(self.bn_2(self.fc_2(x_perp)))\n",
    "        x = F.relu(self.bn_3(self.fc_3(x)))\n",
    "        \n",
    "        num_points = x.shape[1]\n",
    "        x_g = nn.AvgPool1d(num_points)(x.transpose(2, 1)).transpose(2, 1)\n",
    "        \n",
    "        # x_g = torch.unsqueeze(x_g,1)\n",
    "        x_g = torch.tile(x_g,[1,x_perp.shape[1],1])\n",
    "        \n",
    "        x_final = torch.cat([x_perp,x_g],-1)\n",
    "        \n",
    "        R =  F.relu(self.r_bn_1(self.r_fc_1(x_final)))\n",
    "        R =  F.relu(self.r_bn_2(self.r_fc_2(R)))\n",
    "        R =  torch.tanh(self.r_fc_3(R)) * np.pi\n",
    "        \n",
    "        t =  F.relu(self.t_bn_1(self.t_fc_1(x_final)))\n",
    "        t =  F.relu(self.t_bn_2(self.t_fc_2(t)))\n",
    "        t =  self.t_fc_3(t)\n",
    "        \n",
    "        c =  F.relu(self.c_bn_1(self.c_fc_1(x_final)))\n",
    "        c =  F.relu(self.c_bn_2(self.c_fc_2(c)))\n",
    "        c =  self.c_fc_3(c)\n",
    "        \n",
    "        return R, t, c, s_ft, t_ft\n",
    "\n",
    "class adaptive_layer_norm(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, condition_size):\n",
    "        super(adaptive_layer_norm, self).__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "        \n",
    "        self.beta = nn.Linear(condition_size,channels)\n",
    "        self.gamma = nn.Linear(condition_size,channels)\n",
    "     \n",
    "    def forward(self, x, c):\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        gamma = self.gamma(c)\n",
    "        beta = self.beta(c)\n",
    "        \n",
    "        x = x + x * gamma + beta\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PointCloud_Matching_Graph(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PointCloud_Matching_Graph, self).__init__()\n",
    "        self.source_gnn = EncoderProcessorDecoder(6,1,128,128,2,True,5,128)\n",
    "        self.target_gnn = EncoderProcessorDecoder(6,1,128,128,2,True,5,128)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(128, 256)\n",
    "        self.fc_2 = nn.Linear(256, 512)\n",
    "        self.fc_3 = nn.Linear(512, 512)\n",
    "        \n",
    "        self.r_fc_1 = nn.Linear(512, 512)\n",
    "        self.r_fc_2 = nn.Linear(512, 512)\n",
    "        self.r_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        self.t_fc_1 = nn.Linear(512, 512)\n",
    "        self.t_fc_2 = nn.Linear(512, 512)\n",
    "        self.t_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        self.aln1 = adaptive_layer_norm(256,128)\n",
    "        self.aln2 = adaptive_layer_norm(512,128)\n",
    "        self.aln3 = adaptive_layer_norm(512,128)\n",
    "        \n",
    "        self.raln1 = adaptive_layer_norm(512,128)\n",
    "        self.raln2 = adaptive_layer_norm(512,128)\n",
    "        \n",
    "        self.taln1 = adaptive_layer_norm(512,128)\n",
    "        self.taln2 = adaptive_layer_norm(512,128)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        source_graph, target_graph, s_b, t_b = x\n",
    "        \n",
    "        \n",
    "        s_x = self.source_gnn(source_graph)\n",
    "        t_x = self.target_gnn(target_graph)\n",
    "        \n",
    "        s_x = torch_geometric.nn.global_max_pool(s_x,s_b)\n",
    "        t_x = torch_geometric.nn.global_max_pool(t_x,t_b)\n",
    "        \n",
    "        x = F.elu(self.aln1(self.fc_1(s_x),t_x))\n",
    "        x = F.elu(self.aln2(self.fc_2(x),t_x))\n",
    "        x = F.elu(self.aln3(self.fc_3(x),t_x))\n",
    "        \n",
    "        R =  F.elu(self.raln1(self.r_fc_1(x),t_x))\n",
    "        R =  F.elu(self.raln2(self.r_fc_2(R),t_x))\n",
    "        R =  torch.tanh(self.r_fc_3(R)) * np.pi\n",
    "        \n",
    "        t =  F.elu(self.taln1(self.t_fc_1(x),t_x))\n",
    "        t =  F.elu(self.taln2(self.t_fc_2(t),t_x))\n",
    "        t =  self.t_fc_3(t)\n",
    "        \n",
    "        return R, t\n",
    "    \n",
    "class PoseNetFeat(nn.Module):\n",
    "    def __init__(self, large = False):\n",
    "        super(PoseNetFeat, self).__init__()\n",
    "        if large: \n",
    "            self.conv1 = torch.nn.Conv1d(3, 256, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "            self.e_conv1 = torch.nn.Conv1d(32, 256, 1)\n",
    "            self.e_conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "        else:\n",
    "            self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "            self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "            self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(256, 512, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \n",
    "        num_points = x.shape[2]\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        emb = F.relu(self.e_conv1(emb))\n",
    "        pointfeat_1 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        emb = F.relu(self.e_conv2(emb))\n",
    "        pointfeat_2 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv5(pointfeat_2))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        ap_x = torch.nn.AvgPool1d(num_points)(x)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024, 1).repeat(1, 1, num_points)\n",
    "        return torch.cat([pointfeat_1, pointfeat_2, ap_x], 1) #128 + 256 + 1024\n",
    "\n",
    "class PoseNet(nn.Module):\n",
    "    def __init__(self, num_obj, large=False):\n",
    "        super(PoseNet, self).__init__()\n",
    "        # self.num_points = num_points\n",
    "        self.cnn = ModifiedResnet().to(device)\n",
    "        self.feat = PoseNetFeat(large=large)\n",
    "        if large:\n",
    "            self.conv1_r = torch.nn.Conv1d(2048, 1024, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(2048, 1024, 1)\n",
    "            self.conv1_c = torch.nn.Conv1d(2048, 1024, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(1024, 512, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(1024, 512, 1)\n",
    "            self.conv2_c = torch.nn.Conv1d(1024, 512, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv3_c = torch.nn.Conv1d(512, 512, 1)\n",
    "\n",
    "            self.conv4_r = torch.nn.Conv1d(512, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(512, num_obj*3, 1) #translation\n",
    "            self.conv4_c = torch.nn.Conv1d(512, num_obj*1, 1) #confidence\n",
    "        else:\n",
    "            self.conv1_r = torch.nn.Conv1d(1408, 640, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(1408, 640, 1)\n",
    "            self.conv1_c = torch.nn.Conv1d(1408, 640, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(640, 256, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(640, 256, 1)\n",
    "            self.conv2_c = torch.nn.Conv1d(640, 256, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(256, 128, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(256, 128, 1)\n",
    "            self.conv3_c = torch.nn.Conv1d(256, 128, 1)\n",
    "\n",
    "            self.conv4_r = torch.nn.Conv1d(128, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(128, num_obj*3, 1) #translation\n",
    "            self.conv4_c = torch.nn.Conv1d(128, num_obj*1, 1) #confidence\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, img, x, mask, obj):\n",
    "        out_img = self.cnn(img)\n",
    "        num_points = x.shape[1]\n",
    "        out_img = torch.permute(out_img[0],[1,2,0])\n",
    "        emb = torch.unsqueeze(out_img[mask],0).transpose(2,1).contiguous()\n",
    "        \n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        ap_x = self.feat(x, emb)\n",
    "\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))\n",
    "        cx = F.relu(self.conv1_c(ap_x))      \n",
    "\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "        cx = F.relu(self.conv2_c(cx))\n",
    "\n",
    "        rx = F.relu(self.conv3_r(rx))\n",
    "        tx = F.relu(self.conv3_t(tx))\n",
    "        cx = F.relu(self.conv3_c(cx))\n",
    "\n",
    "        rx = self.conv4_r(rx).view(1, self.num_obj, 4, num_points)\n",
    "        tx = self.conv4_t(tx).view(1, self.num_obj, 3, num_points)\n",
    "        cx = self.conv4_c(cx).view(1, self.num_obj, 1, num_points)\n",
    "        \n",
    "        out_rx = rx[:,obj,:,:]\n",
    "        out_tx = tx[:,obj,:,:]\n",
    "        out_cx = cx[:,obj,:,:]\n",
    "        \n",
    "        out_rx = out_rx.contiguous().transpose(2, 1).contiguous()\n",
    "        out_cx = out_cx.contiguous().transpose(2, 1).contiguous()\n",
    "        out_tx = out_tx.contiguous().transpose(2, 1).contiguous()\n",
    "        \n",
    "        return out_rx, out_tx, out_cx, emb.detach()\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling \n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "        \n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "        \n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        softmax = nn.functional.softmax\n",
    "        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
    "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class PoseNet_Attn(nn.Module):\n",
    "    def __init__(self, num_obj, large=False):\n",
    "        super(PoseNet_Attn, self).__init__()\n",
    "        # self.num_points = num_points\n",
    "        self.cnn = ModifiedResnet().to(device)\n",
    "        self.feat = PoseNetFeat(large=large)\n",
    "        if large:\n",
    "            self.conv1_r = torch.nn.Conv1d(2048, 1024, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(2048, 1024, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(1024, 512, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(1024, 512, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(512, 512, 1)\n",
    "            \n",
    "            self.r_attn_pool = AttentionPooling(512)\n",
    "            self.t_attn_pool = AttentionPooling(512)\n",
    "            \n",
    "            self.conv4_r = torch.nn.Conv1d(512, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(512, num_obj*3, 1) #translation\n",
    "        else:\n",
    "            \n",
    "#             self.lrbl_e_r = torch.nn.Parameter(torch.randn(128))\n",
    "#             self.lrbl_e_r.requires_grad = True\n",
    "            \n",
    "#             self.lrbl_e_t = torch.nn.Parameter(torch.randn(128))\n",
    "#             self.lrbl_e_t.requires_grad = True\n",
    "            \n",
    "#             self.TEL_r = TransformerEncoderLayer_NoNorm(128,8)\n",
    "#             self.transf_r = nn.TransformerEncoder(self.TEL_r, num_layers=6)\n",
    "            \n",
    "#             self.TEL_t = TransformerEncoderLayer_NoNorm(128,8)\n",
    "#             self.transf_t = nn.TransformerEncoder(self.TEL_t, num_layers=6)\n",
    "            \n",
    "            self.conv1_r = torch.nn.Conv1d(1408, 640, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(1408, 640, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(640, 256, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(640, 256, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(256, 128, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(256, 128, 1)\n",
    "            \n",
    "            self.r_attn_pool = AttentionPooling(128)\n",
    "            self.t_attn_pool = AttentionPooling(128)\n",
    "\n",
    "            self.conv4_r = torch.nn.Conv1d(128, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(128, num_obj*3, 1) #translation\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, img, x, mask, obj):\n",
    "        out_img = self.cnn(img)\n",
    "        num_points = x.shape[1]\n",
    "        out_img = torch.permute(out_img[0],[1,2,0])\n",
    "        emb = torch.unsqueeze(out_img[mask],0).transpose(2,1).contiguous()\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        ap_x = self.feat(x, emb)\n",
    "\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))\n",
    "\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "\n",
    "        rx = F.relu(self.conv3_r(rx))\n",
    "        tx = F.relu(self.conv3_t(tx))\n",
    "        \n",
    "        rx = self.r_attn_pool(rx.transpose(2,1)).unsqueeze(-1)\n",
    "        tx = self.t_attn_pool(tx.transpose(2,1)).unsqueeze(-1)\n",
    "\n",
    "#         rx = self.transf_r(torch.cat([self.lrbl_e_r.unsqueeze(0).unsqueeze(0).repeat([bs,1,1]), rx.transpose(2,1)],1))[:,0:1,:].transpose(2,1)\n",
    "#         tx = self.transf_t(torch.cat([self.lrbl_e_t.unsqueeze(0).unsqueeze(0).repeat([bs,1,1]), tx.transpose(2,1)],1))[:,0:1,:].transpose(2,1)\n",
    "\n",
    "        rx = self.conv4_r(rx).view(bs, self.num_obj, 4)\n",
    "        tx = self.conv4_t(tx).view(bs, self.num_obj, 3)\n",
    "        \n",
    "        out_rx = rx[:,obj:obj+1,:]\n",
    "        out_tx = tx[:,obj:obj+1,:]\n",
    "        \n",
    "        \n",
    "        return out_rx, out_tx, None, emb.detach()\n",
    "    \n",
    "class PoseNet_Encoder(nn.Module):\n",
    "    def __init__(self, large = False):\n",
    "        super(PoseNet_Encoder, self).__init__()\n",
    "        if large: \n",
    "            self.conv1 = torch.nn.Conv1d(3, 256, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "#             self.e_conv1 = torch.nn.Conv1d(32, 256, 1)\n",
    "#             self.e_conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "        else:\n",
    "            self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "#             self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "#             self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(128, 128, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(128, 128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        num_points = x.shape[2]\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "#         emb = F.relu(self.e_conv1(emb))\n",
    "#         pointfeat_1 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "#         emb = F.relu(self.e_conv2(emb))\n",
    "#         pointfeat_2 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        ap_x = torch.nn.AvgPool1d(num_points)(x)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 128, 1)\n",
    "        return ap_x    \n",
    "\n",
    "class PoseRefineNetFeat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseRefineNetFeat, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "        self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "        self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        \n",
    "        self.ref_conv1 = torch.nn.Conv1d(128, 128, 1)\n",
    "        self.ref_conv2 = torch.nn.Conv1d(128, 128, 1)\n",
    "\n",
    "        self.conv5 = torch.nn.Conv1d(256+128, 512, 1)\n",
    "        self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        num_points = x.shape[2]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        emb = F.relu(self.e_conv1(emb))\n",
    "#         ref_e = F.relu(self.ref_conv1(ref_e)).repeat(1, 1, num_points)\n",
    "        pointfeat_1 = torch.cat([x, emb], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        emb = F.relu(self.e_conv2(emb))\n",
    "#         ref_e = F.relu(self.ref_conv2(ref_e))\n",
    "        pointfeat_2 = torch.cat([x, emb], dim=1)\n",
    "\n",
    "        pointfeat_3 = torch.cat([pointfeat_1, pointfeat_2], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv5(pointfeat_3))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        ap_x = torch.nn.AvgPool1d(num_points)(x)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024)\n",
    "        return ap_x\n",
    "\n",
    "class PoseRefineNet(nn.Module):\n",
    "    def __init__(self, num_obj):\n",
    "        super(PoseRefineNet, self).__init__()\n",
    "        self.feat = PoseRefineNetFeat()\n",
    "#         self.ref_enc = PoseNet_Encoder()\n",
    "        \n",
    "        self.conv1_r = torch.nn.Linear(1024, 512)\n",
    "        self.conv1_t = torch.nn.Linear(1024, 512)\n",
    "\n",
    "        self.conv2_r = torch.nn.Linear(512, 128)\n",
    "        self.conv2_t = torch.nn.Linear(512, 128)\n",
    "\n",
    "        self.conv3_r = torch.nn.Linear(128, num_obj*4) #quaternion\n",
    "        self.conv3_t = torch.nn.Linear(128, num_obj*3) #translation\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, x, emb, obj):\n",
    "        \n",
    "        num_points = x.shape[1]\n",
    "        bs = x.shape[0]\n",
    "#         ref_e = self.ref_enc(ref_x.transpose(2, 1).contiguous())\n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        ap_x = self.feat(x, emb)\n",
    "\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))   \n",
    "\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "\n",
    "        rx = self.conv3_r(rx).view(bs, self.num_obj, 4)\n",
    "        tx = self.conv3_t(tx).view(bs, self.num_obj, 3)\n",
    "\n",
    "        out_rx = rx[:,obj,:]\n",
    "        out_tx = tx[:,obj,:]\n",
    "\n",
    "        return out_rx, out_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8753b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_posnet(i, aug = True):\n",
    "    sym = [11,10]\n",
    "    pc,c,obj_id,R,t,idx = point_cloud_pairs[i]\n",
    "    mask = (merged_masks[idx] == obj_to_mask_value[obj_id])\n",
    "    depth = depth_imgs[idx]\n",
    "    rgb = np.copy(rgbs[idx])/255.0\n",
    "#     rgb[np.logical_not(mask)] = 0.\n",
    "    \n",
    "    mask = (depth[:,:,-1] > 0) * mask\n",
    "    if aug:\n",
    "        angle = 0.0\n",
    "    else:\n",
    "        angle = 0.0\n",
    "    rgb = np.transpose(torchvision.transforms.functional.rotate(torch.Tensor(np.transpose(rgb,[2,0,1])),angle).detach().numpy(),[1,2,0])\n",
    "    mask = torchvision.transforms.functional.rotate(torch.Tensor(np.transpose(np.expand_dims(mask,-1),[2,0,1]).astype(np.float32)),angle).detach().numpy()[0].astype(bool)\n",
    "    depth = np.transpose(torchvision.transforms.functional.rotate(torch.Tensor(np.transpose(depth,[2,0,1])),angle).detach().numpy(),[1,2,0])\n",
    "    \n",
    "    if mask.sum()>500:\n",
    "        ridx = np.random.choice(mask.sum(),size=mask.sum()-500,replace=False)\n",
    "        i1,i2 = np.where(mask)\n",
    "        mask[i1[ridx],i2[ridx]] = False\n",
    "    pc = depth[mask]\n",
    "    \n",
    "    \n",
    "    RR = roma.rotvec_to_rotmat(torch.Tensor([0,0,-angle*np.pi/180])).detach().numpy()\n",
    "    pc = (RR@pc.T).T\n",
    "    \n",
    "    obj_class = obj_id_list.index(obj_id)\n",
    "    target_pc = reference_point_clouds[obj_id_list.index(obj_id)][0]\n",
    "    if target_pc.shape[0] > 500:\n",
    "        target_pc = target_pc[np.random.choice(target_pc.shape[0],size=500, replace=False)]\n",
    "    target = (RR@((np.reshape(R,[3,3])@target_pc.T).T + t).T).T\n",
    "    \n",
    "    if aug:\n",
    "        rgb = rgb + np.random.uniform(low =-0.3, high=0.3,size=rgb.shape)\n",
    "        rgb = np.minimum(rgb,1.0)\n",
    "        rgb = np.maximum(rgb,0.0)\n",
    "#         pc = pc + np.random.uniform(low =-2.0, high=2.0,size=pc.shape)\n",
    "    return rgb,pc,mask,obj_class, R, t, obj_id in sym, diameters[obj_id_list.index(obj_id)], target, target_pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4cebb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = PoseNet_Attn(8).to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(pose_model.parameters(), lr=learning_rate)\n",
    "decay_stepper = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.6)\n",
    "\n",
    "w = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1429d687",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_pairs_train = []\n",
    "point_cloud_pairs_test = []\n",
    "for i,pcp in enumerate(point_cloud_pairs):\n",
    "    if pcp[-1] in train:\n",
    "        point_cloud_pairs_train.append(i)\n",
    "    elif pcp[-1] in test:\n",
    "        point_cloud_pairs_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbc6978",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ADD = 0.0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    pose_model.eval()\n",
    "    if epoch>0:\n",
    "        for i in tqdm(point_cloud_pairs_test):\n",
    "            cc+= 1\n",
    "\n",
    "            rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i,aug=False)\n",
    "\n",
    "            if pc.shape[0]>0:\n",
    "                R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "                t = t\n",
    "                R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "                Rs = roma.unitquat_to_rotmat(R[:,0,:])\n",
    "                t = t[:,0,:]\n",
    "                transformed = torch.bmm(Rs,torch.Tensor(np.expand_dims(target_pc,0)).transpose(1,2).to(device)).transpose(1,2) + t\n",
    "                target = torch.Tensor(target).to(device)\n",
    "\n",
    "                if is_sym:\n",
    "                    L_p = torch.mean(torch.min(torch.cdist(transformed,torch.unsqueeze(target,0)),dim=-1)[0])\n",
    "                else:\n",
    "                    L_p = torch.mean(torch.norm(transformed[0] - target,dim=-1),dim=-1)\n",
    "\n",
    "                if L_p <= 0.1 * d:\n",
    "                    ADD += 1\n",
    "\n",
    "                l_ov += L_p.cpu().detach().numpy()\n",
    "        if ADD >= best_ADD:\n",
    "            best_ADD = ADD\n",
    "            if epoch > 0:\n",
    "                torch.save(pose_model,'best_linemode_new.pt')\n",
    "        print('Validation Lp: %.7f, ADD(-S): %.7f'%(l_ov/cc,ADD/cc))\n",
    "    \n",
    "    shuffle = np.random.choice(int(point_cloud_pairs.shape[0] * 0.9), size = int(point_cloud_pairs.shape[0] * 0.9),replace=False)\n",
    "    random.shuffle(point_cloud_pairs_train)\n",
    "    prog = tqdm(point_cloud_pairs_train)\n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    for i in prog:\n",
    "        cc+= 1\n",
    "        if epoch == 0 and cc<=1000:\n",
    "            optimizer.param_groups[0]['lr'] = 1e-5 * 10**(cc/1000.)\n",
    "        optimizer.zero_grad()\n",
    "        pose_model = pose_model.train()\n",
    "        \n",
    "        rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i)\n",
    "        if pc.shape[0]>0:\n",
    "            R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "            t = t\n",
    "            R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "            Rs = roma.unitquat_to_rotmat(R)\n",
    "            transformed = torch.bmm(Rs[0],torch.tile(torch.Tensor(np.expand_dims(target_pc,0)),[Rs.shape[1],1,1]).transpose(1,2).to(device)).transpose(1,2) + torch.unsqueeze(t[0],1)\n",
    "            target = torch.Tensor(target).to(device)\n",
    "            \n",
    "            if is_sym:\n",
    "                L_p = torch.cdist(transformed.reshape(-1,3),target).min(-1)[0].reshape(transformed.shape[0],transformed.shape[1]).mean(-1)\n",
    "                \n",
    "            else:\n",
    "                target = torch.unsqueeze(target,0)\n",
    "                \n",
    "                L_p = torch.mean(torch.norm(transformed - target,dim=-1),dim=-1)\n",
    "\n",
    "            if L_p[0].cpu().detach().numpy() <= 0.1 * d:\n",
    "                ADD += 1\n",
    "\n",
    "            loss = torch.mean(L_p)\n",
    "\n",
    "            l_ov += torch.min(L_p).cpu().detach().numpy()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            prog.set_postfix_str('ADD(-S): %.7f, overall_loss: %.7f, loss: %.7f, Lp: %.7f'%(ADD/cc,l_ov/cc,loss.cpu().detach().numpy(),torch.min(L_p).cpu().detach().numpy()))\n",
    "        \n",
    "    decay_stepper.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b43292",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_model = PoseRefineNet(8).to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "optimizer_r = torch.optim.Adam(refine_model.parameters(), lr=learning_rate)\n",
    "decay_stepper_r = torch.optim.lr_scheduler.StepLR(optimizer_r, 10, gamma=0.6)\n",
    "\n",
    "pose_model = pose_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0ebd48-9cbb-4c5c-bab4-460184029609",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ADD = 0.75\n",
    "for epoch in range(epochs):\n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    refine_model = refine_model.eval()\n",
    "    if epoch>0:\n",
    "        for i in tqdm(point_cloud_pairs_test):\n",
    "            cc+= 1\n",
    "\n",
    "            rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i,aug=False)\n",
    "            if pc.shape[0]>0:\n",
    "                torch_rgb = torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device)\n",
    "                torch_pc = torch.Tensor(np.expand_dims(pc,0)).to(device)\n",
    "                torch_target = torch.Tensor(target).to(device)\n",
    "\n",
    "                R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "                t = t\n",
    "                R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "                Rs = roma.unitquat_to_rotmat(R)[:,0]\n",
    "                Rs_i = roma.rotmat_inverse(Rs)\n",
    "\n",
    "                new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - t).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "                R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "                R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "                R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "                R_final = torch.bmm(R_r,Rs)\n",
    "                T_final = T_r + t[:,0]\n",
    "                \n",
    "                Rs_i = roma.rotmat_inverse(R_final)\n",
    "                new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "                R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "                R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "                R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "                R_final = torch.bmm(R_r,R_final)\n",
    "                T_final += T_r\n",
    "\n",
    "                Rs_i = roma.rotmat_inverse(R_final)\n",
    "                new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "                R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "                R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "                R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "                R_final = torch.bmm(R_r,R_final)\n",
    "                T_final += T_r\n",
    "                \n",
    "                transformed = torch.bmm(R_final, torch.Tensor(target_pc.T).to(device).unsqueeze(0).repeat([R_final.shape[0],1,1])).transpose(1,2) + T_final.unsqueeze(1)\n",
    "\n",
    "                if is_sym:\n",
    "                    L_p = torch.cdist(transformed.reshape(-1,3),torch_target).min(-1)[0].reshape(transformed.shape[0],transformed.shape[1]).mean(-1)\n",
    "\n",
    "                else:\n",
    "                    L_p = torch.norm(transformed - torch_target.unsqueeze(0),dim=-1).mean(-1)\n",
    "\n",
    "                if L_p <= 0.1 * d:\n",
    "                        ADD += 1\n",
    "\n",
    "                l_ov += L_p.cpu().detach().numpy()\n",
    "            \n",
    "        if ADD >= best_ADD:\n",
    "            best_ADD = ADD\n",
    "            if epoch > 0:\n",
    "                torch.save(refine_model,'best_linemode_refine_final.pt')\n",
    "        print('Validation Lp: %.7f, ADD(-S): %.7f'%(l_ov/cc,ADD/cc))\n",
    "    \n",
    "    random.shuffle(point_cloud_pairs_train)\n",
    "    prog = tqdm(point_cloud_pairs_train)\n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    \n",
    "    for i in prog:\n",
    "        cc+= 1\n",
    "        \n",
    "        optimizer_r.zero_grad()\n",
    "        refine_model = refine_model.train()\n",
    "        \n",
    "        rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i)\n",
    "        if pc.shape[0]>0:\n",
    "            torch_rgb = torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device)\n",
    "            torch_pc = torch.Tensor(np.expand_dims(pc,0)).to(device)\n",
    "            torch_target = torch.Tensor(target).to(device)\n",
    "\n",
    "            R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "            t = t\n",
    "            R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "            Rs = roma.unitquat_to_rotmat(R)[:,0]\n",
    "            Rs_i = roma.rotmat_inverse(Rs)\n",
    "\n",
    "            new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - t).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "            R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "            R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "            R_r = roma.unitquat_to_rotmat(R_r)\n",
    "        \n",
    "            R_final = torch.bmm(R_r,Rs)\n",
    "            T_final = T_r + t[:,0]\n",
    "            \n",
    "            Rs_i = roma.rotmat_inverse(R_final)\n",
    "            new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "            R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "            R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "            R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "            R_final = torch.bmm(R_r,R_final)\n",
    "            T_final += T_r\n",
    "\n",
    "            Rs_i = roma.rotmat_inverse(R_final)\n",
    "            new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "            R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "            R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "            R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "            R_final = torch.bmm(R_r,R_final)\n",
    "            T_final += T_r\n",
    "            \n",
    "            transformed = torch.bmm(R_final, torch.Tensor(target_pc.T).to(device).unsqueeze(0).repeat([R_final.shape[0],1,1])).transpose(1,2) + T_final.unsqueeze(1)\n",
    "            \n",
    "            if is_sym:\n",
    "                L_p = torch.cdist(transformed.reshape(-1,3),torch_target).min(-1)[0].reshape(transformed.shape[0],transformed.shape[1]).mean(-1)\n",
    "\n",
    "            else:\n",
    "                L_p = torch.norm(transformed - torch_target.unsqueeze(0),dim=-1).mean(-1)\n",
    "                \n",
    "            if L_p <= 0.1 * d:\n",
    "                    ADD += 1\n",
    "            \n",
    "            l_ov += L_p.cpu().detach().numpy()\n",
    "                \n",
    "            loss = L_p.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_r.step()\n",
    "\n",
    "            prog.set_postfix_str('ADD(-S): %.7f, overall_loss: %.7f, loss: %.7f, Lp: %.7f'%(ADD/cc,l_ov/cc,loss.cpu().detach().numpy(),torch.min(L_p).cpu().detach().numpy()))\n",
    "        \n",
    "    decay_stepper_r.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79bb661-405a-4579-8102-1abac674e40c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
