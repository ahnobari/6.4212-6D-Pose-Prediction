{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1398475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ['HDF5_USE_FILE_LOCKING'] = 'FALSE'\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "from glob import glob\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import open3d as o3d\n",
    "\n",
    "import trimesh\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "import yaml\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import h5py\n",
    "\n",
    "import random\n",
    "\n",
    "import roma\n",
    "\n",
    "# from epd import *\n",
    "from tqdm.autonotebook import tqdm\n",
    "import scipy\n",
    "\n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35ab24c",
   "metadata": {},
   "source": [
    "## Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21928347",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = h5py.File('processed_data_subs.h5', 'r')\n",
    "rgbs = hf['rgbs'][:]\n",
    "depth_imgs = hf['depth_imgs'][:]\n",
    "merged_masks = hf['merged_masks'][:]\n",
    "point_cloud_pairs = np.load('point_cloud_pairs.npy',allow_pickle=True)\n",
    "gt = np.load('custom_gt.npy',allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da538e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_mask_value = {14: 0, 28: 1, 42: 2, 56: 3, 70: 4, 85: 5, 99: 6, 113: 7, 127: 8, 141: 9, 155: 10, 170: 11, 184: 12, 198: 13, 212: 14, 226: 15, 240: 16, 255: 17}\n",
    "obj_to_mask_value = {v: k for k, v in obj_mask_value.items()}\n",
    "\n",
    "reference_point_clouds = np.load('reference_point_clouds_custom.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298860f4",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be496550",
   "metadata": {},
   "source": [
    "### Reference Point Clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b40c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.Scene()\n",
    "for i,pc in enumerate(reference_point_clouds):\n",
    "    scene.add_geometry(trimesh.points.PointCloud(pc[0]*1000 + [i%6 * 300, i//6 * 300 ,0],colors=pc[1]))\n",
    "scene.camera.z_far = 10\n",
    "scene.show(viewer='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2015cc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "diameters = []\n",
    "for i in range(len(reference_point_clouds)):\n",
    "    diameters.append(float(torch.cdist(torch.Tensor(reference_point_clouds[i][0]),torch.Tensor(reference_point_clouds[i][0])).max().detach().numpy())*1000.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4551b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_list = []\n",
    "for i in gt.keys():\n",
    "    idx_list += [i] * len(gt[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d678e6",
   "metadata": {},
   "source": [
    "### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d2a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "rind = np.random.choice(8000)\n",
    "point_cloud_pair = point_cloud_pairs[rind]\n",
    "rgb = rgbs[idx_list[rind]]\n",
    "merged_mask = merged_masks[idx_list[rind]]\n",
    "depth_img = depth_imgs[idx_list[rind]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2639da",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(rgb)\n",
    "plt.axis('off')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(merged_mask,cmap='Greys_r')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7e7c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.Scene()\n",
    "dp_i = np.copy(depth_img[:,:,0:3])\n",
    "dp_i[np.isnan(dp_i)] = 0.0\n",
    "scene.add_geometry(trimesh.points.PointCloud(dp_i.reshape([-1,3])*1000.0,colors=rgb.reshape([-1,3])))\n",
    "scene.camera.z_far = 10\n",
    "scene.show(viewer='notebook')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c897f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene = trimesh.Scene()\n",
    "# obj_id_list = list(id_to_name.keys())\n",
    "\n",
    "pc_source = trimesh.points.PointCloud(point_cloud_pair[0],colors=point_cloud_pair[1])\n",
    "scene.add_geometry(pc_source)\n",
    "\n",
    "r_pc = np.copy(reference_point_clouds[point_cloud_pair[2]-1][0])\n",
    "r_pc = (np.reshape(point_cloud_pair[3],(3,3)) @ r_pc.T).T + point_cloud_pair[4]\n",
    "pc_target = trimesh.points.PointCloud(r_pc,colors=[0,255,0])\n",
    "scene.add_geometry(pc_target)\n",
    "\n",
    "scene.camera.z_far = 10\n",
    "scene.show(viewer='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67b8cf",
   "metadata": {},
   "source": [
    "## Instance Segmentaion Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8824c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ManipulationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = list(range(len(rgbs)))\n",
    "        self.masks = list(range(len(rgbs)))\n",
    "        \n",
    "        self.obj_id_dict = {0: '055_baseball', 1: '056_tennis_ball', 2: '072-a_toy_airplane', 3: '019_pitcher_base', 4: '040_large_marker', 5: '021_bleach_cleanser', 6: '077_rubiks_cube', 7: '048_hammer', 8: '008_pudding_box', 9: '053_mini_soccer_ball', 10: '011_banana', 11: '006_mustard_bottle', 12: '013_apple', 13: '029_plate', 14: '035_power_drill', 15: '043_phillips_screwdriver', 16: '032_knife', 17: '042_adjustable_wrench'}\n",
    "        self.obj_mask_value = {14: 0, 28: 1, 42: 2, 56: 3, 70: 4, 85: 5, 99: 6, 113: 7, 127: 8, 141: 9, 155: 10, 170: 11, 184: 12, 198: 13, 212: 14, 226: 15, 240: 16, 255: 17}\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        # img_path = os.path.join(self.root, \"rgb\", self.imgs[idx])\n",
    "        # mask_path = os.path.join(self.root, \"merged_masks\", self.masks[idx])\n",
    "        img = torch.Tensor(np.transpose(rgbs[self.imgs[idx]],[2,0,1]))\n",
    "        # note that we haven't converted the mask to RGB,\n",
    "        # because each color corresponds to a different instance\n",
    "        # with 0 being background\n",
    "        mask = merged_masks[self.masks[idx]]\n",
    "\n",
    "        # instances are encoded as different colors\n",
    "        obj_ids = np.unique(mask)\n",
    "        # first id is the background, so remove it\n",
    "        obj_ids = obj_ids[1:]\n",
    "\n",
    "        # split the color-encoded mask into a set\n",
    "        # of binary masks\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get bounding box coordinates for each mask\n",
    "        num_objs = len(obj_ids)\n",
    "        \n",
    "        labels = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        boxes = []\n",
    "        for i in range(num_objs):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "            labels[i] = self.obj_mask_value[obj_ids[i]] + 1\n",
    "\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        \n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"masks\"] = masks\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        img = img.to(torch.float32)/255.0\n",
    "        \n",
    "        Normalizer = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        \n",
    "        return Normalizer(img), target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88ca993",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "    # get the number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    # Stop here if you are fine-tunning Faster-RCNN\n",
    "\n",
    "    # now get the number of input features for the mask classifier\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    # and replace the mask predictor with a new one\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,\n",
    "                                                       hidden_layer,\n",
    "                                                       num_classes)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900b076",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = list(range(900))\n",
    "test = list(range(900,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef32d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    \n",
    "    return T.Compose(transforms)\n",
    "\n",
    "\n",
    "dataset = ManipulationDataset('../manipulation/dataset/', get_transform(train=True))\n",
    "dataset_test = ManipulationDataset('../manipulation/dataset/', get_transform(train=False))\n",
    "\n",
    "torch.manual_seed(1)\n",
    "dataset = torch.utils.data.Subset(dataset, train)\n",
    "dataset_test = torch.utils.data.Subset(dataset_test, test)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=16, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "data_loader_test = torch.utils.data.DataLoader(\n",
    "    dataset_test, batch_size=16, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1e273f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(device) if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_classes = 19\n",
    "\n",
    "model = build_model(num_classes)\n",
    "model.to(device)\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(params, lr=1e-3)\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                               step_size=30,\n",
    "                                               gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e930e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs\n",
    "if os.path.exists('mask-rcnn-custom-data.pt'):\n",
    "    model = torch.load('mask-rcnn-custom-data.pt')\n",
    "else:\n",
    "    num_epochs = 100\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=100000, epoch_length=int(split/16))\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_test, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308d3dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "CLASS_NAMES = ['__background__', \n",
    "               '055_baseball',\n",
    "               '056_tennis_ball',\n",
    "               '072-a_toy_airplane',\n",
    "               '019_pitcher_base',\n",
    "               '040_large_marker',\n",
    "               '021_bleach_cleanser',\n",
    "               '077_rubiks_cube',\n",
    "               '048_hammer',\n",
    "               '008_pudding_box',\n",
    "               '053_mini_soccer_ball',\n",
    "               '011_banana',\n",
    "               '006_mustard_bottle',\n",
    "               '013_apple',\n",
    "               '029_plate',\n",
    "               '035_power_drill',\n",
    "               '043_phillips_screwdriver',\n",
    "               '032_knife',\n",
    "               '042_adjustable_wrench']\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad670cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coloured_mask(mask):\n",
    "    \"\"\"\n",
    "    random_colour_masks\n",
    "      parameters:\n",
    "        - image - predicted masks\n",
    "      method:\n",
    "        - the masks of each predicted object is given random colour for visualization\n",
    "    \"\"\"\n",
    "    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n",
    "    r = np.zeros_like(mask).astype(np.uint8)\n",
    "    g = np.zeros_like(mask).astype(np.uint8)\n",
    "    b = np.zeros_like(mask).astype(np.uint8)\n",
    "    r[mask == 1], g[mask == 1], b[mask == 1] = colours[random.randrange(0,10)]\n",
    "    coloured_mask = np.stack([r, g, b], axis=2)\n",
    "    return coloured_mask\n",
    "\n",
    "def get_prediction(img, confidence):\n",
    "    \"\"\"\n",
    "    get_prediction\n",
    "      parameters:\n",
    "        - img_path - path of the input image\n",
    "        - confidence - threshold to keep the prediction or not\n",
    "      method:\n",
    "        - Image is obtained from the image path\n",
    "        - the image is converted to image tensor using PyTorch's Transforms\n",
    "        - image is passed through the model to get the predictions\n",
    "        - masks, classes and bounding boxes are obtained from the model and soft masks are made binary(0 or 1) on masks\n",
    "          ie: eg. segment of cat is made 1 and rest of the image is made 0\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    img = img.to(device)\n",
    "    pred = model([img])\n",
    "    pred_score = list(pred[0]['scores'].detach().cpu().numpy())\n",
    "    pred_t = [pred_score.index(x) for x in pred_score if x>confidence][-1]\n",
    "    masks = (pred[0]['masks']>0.5).squeeze().detach().cpu().numpy()\n",
    "    pred_class = [CLASS_NAMES[i] for i in list(pred[0]['labels'].cpu().numpy())]\n",
    "    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().cpu().numpy())]\n",
    "    masks = masks[:pred_t+1]\n",
    "    pred_boxes = pred_boxes[:pred_t+1]\n",
    "    pred_class = pred_class[:pred_t+1]\n",
    "    return masks, pred_boxes, pred_class\n",
    "\n",
    "def segment_instance(img, rgb_img, confidence=0.5, rect_th=2, text_size=0.75, text_th=2):\n",
    "    \"\"\"\n",
    "    segment_instance\n",
    "      parameters:\n",
    "        - img_path - path to input image\n",
    "        - confidence- confidence to keep the prediction or not\n",
    "        - rect_th - rect thickness\n",
    "        - text_size\n",
    "        - text_th - text thickness\n",
    "      method:\n",
    "        - prediction is obtained by get_prediction\n",
    "        - each mask is given random color\n",
    "        - each mask is added to the image in the ration 1:0.8 with opencv\n",
    "        - final output is displayed\n",
    "    \"\"\"\n",
    "    masks, boxes, pred_cls = get_prediction(img, confidence)\n",
    "    img = rgb_img\n",
    "    for i in range(len(masks)):\n",
    "        rgb_mask = get_coloured_mask(masks[i])\n",
    "        img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n",
    "        cv2.rectangle(img, [int(boxes[i][0][0]),int(boxes[i][0][1])],[int(boxes[i][1][0]),int(boxes[i][1][1])],color=(0, 255, 0), thickness=rect_th)\n",
    "        cv2.putText(img,pred_cls[i], [int(boxes[i][0][0]),int(boxes[i][0][1])], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n",
    "    plt.figure(figsize=(10,15))\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3186b877",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_i = np.random.choice(len(test))\n",
    "segment_instance(dataset_test[rand_i][0],rgbs[test[rand_i]],confidence=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb798568",
   "metadata": {},
   "source": [
    "## Pose Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ddd84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Any, Union, Callable\n",
    "\n",
    "class TransformerEncoderLayer_NoNorm(nn.Module):\n",
    "    r\"\"\"TransformerEncoderLayer is made up of self-attn and feedforward network.\n",
    "    This standard encoder layer is based on the paper \"Attention Is All You Need\".\n",
    "    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n",
    "    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in\n",
    "    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement\n",
    "    in a different way during application.\n",
    "\n",
    "    Args:\n",
    "        d_model: the number of expected features in the input (required).\n",
    "        nhead: the number of heads in the multiheadattention models (required).\n",
    "        dim_feedforward: the dimension of the feedforward network model (default=2048).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        activation: the activation function of the intermediate layer, can be a string\n",
    "            (\"relu\" or \"gelu\") or a unary callable. Default: relu\n",
    "        layer_norm_eps: the eps value in layer normalization components (default=1e-5).\n",
    "        batch_first: If ``True``, then the input and output tensors are provided\n",
    "            as (batch, seq, feature). Default: ``False`` (seq, batch, feature).\n",
    "        norm_first: if ``True``, layer norm is done prior to attention and feedforward\n",
    "            operations, respectively. Otherwise it's done after. Default: ``False`` (after).\n",
    "\n",
    "    Examples::\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "        >>> src = torch.rand(10, 32, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Alternatively, when ``batch_first`` is ``True``:\n",
    "        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "        >>> src = torch.rand(32, 10, 512)\n",
    "        >>> out = encoder_layer(src)\n",
    "\n",
    "    Fast path:\n",
    "        forward() will use a special optimized implementation if all of the following\n",
    "        conditions are met:\n",
    "\n",
    "        - Either autograd is disabled (using ``torch.inference_mode`` or ``torch.no_grad``) or no tensor\n",
    "          argument ``requires_grad``\n",
    "        - training is disabled (using ``.eval()``)\n",
    "        - batch_first is ``True`` and the input is batched (i.e., ``src.dim() == 3``)\n",
    "        - activation is one of: ``\"relu\"``, ``\"gelu\"``, ``torch.functional.relu``, or ``torch.functional.gelu``\n",
    "        - at most one of ``src_mask`` and ``src_key_padding_mask`` is passed\n",
    "        - if src is a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_, neither ``src_mask``\n",
    "          nor ``src_key_padding_mask`` is passed\n",
    "        - the two ``LayerNorm`` instances have a consistent ``eps`` value (this will naturally be the case\n",
    "          unless the caller has manually modified one without modifying the other)\n",
    "\n",
    "        If the optimized implementation is in use, a\n",
    "        `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ can be\n",
    "        passed for ``src`` to represent padding more efficiently than using a padding\n",
    "        mask. In this case, a `NestedTensor <https://pytorch.org/docs/stable/nested.html>`_ will be\n",
    "        returned, and an additional speedup proportional to the fraction of the input that\n",
    "        is padding can be expected.\n",
    "    \"\"\"\n",
    "    __constants__ = ['batch_first', 'norm_first']\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, dim_feedforward: int = 2048, dropout: float = 0.1,\n",
    "                 activation: Union[str, Callable[[torch.Tensor], torch.Tensor]] = F.relu,\n",
    "                 layer_norm_eps: float = 1e-5, batch_first: bool = False, norm_first: bool = False,\n",
    "                 device=None, dtype=None) -> None:\n",
    "        factory_kwargs = {'device': device, 'dtype': dtype}\n",
    "        super(TransformerEncoderLayer_NoNorm, self).__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=batch_first,\n",
    "                                            **factory_kwargs)\n",
    "        # Implementation of Feedforward model\n",
    "        self.linear1 = nn.Linear(d_model, dim_feedforward, **factory_kwargs)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(dim_feedforward, d_model, **factory_kwargs)\n",
    "\n",
    "        self.norm_first = norm_first\n",
    "#         self.norm1 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "#         self.norm2 = LayerNorm(d_model, eps=layer_norm_eps, **factory_kwargs)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "        # Legacy string support for activation function.\n",
    "        if isinstance(activation, str):\n",
    "            activation = _get_activation_fn(activation)\n",
    "\n",
    "        # We can't test self.activation in forward() in TorchScript,\n",
    "        # so stash some information about it instead.\n",
    "        if activation is F.relu or isinstance(activation, torch.nn.ReLU):\n",
    "            self.activation_relu_or_gelu = 1\n",
    "        elif activation is F.gelu or isinstance(activation, torch.nn.GELU):\n",
    "            self.activation_relu_or_gelu = 2\n",
    "        else:\n",
    "            self.activation_relu_or_gelu = 0\n",
    "        self.activation = activation\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        super(TransformerEncoderLayer_NoNorm, self).__setstate__(state)\n",
    "        if not hasattr(self, 'activation'):\n",
    "            self.activation = F.relu\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor, src_mask: Optional[torch.Tensor] = None,\n",
    "                src_key_padding_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "        r\"\"\"Pass the input through the encoder layer.\n",
    "\n",
    "        Args:\n",
    "            src: the sequence to the encoder layer (required).\n",
    "            src_mask: the mask for the src sequence (optional).\n",
    "            src_key_padding_mask: the mask for the src keys per batch (optional).\n",
    "\n",
    "        Shape:\n",
    "            see the docs in Transformer class.\n",
    "        \"\"\"\n",
    "\n",
    "        if src_key_padding_mask is not None:\n",
    "            _skpm_dtype = src_key_padding_mask.dtype\n",
    "            if _skpm_dtype != torch.bool and not torch.is_floating_point(src_key_padding_mask):\n",
    "                raise AssertionError(\n",
    "                    \"only bool and floating types of key_padding_mask are supported\")\n",
    "        # see Fig. 1 of https://arxiv.org/pdf/2002.04745v1.pdf\n",
    "        why_not_sparsity_fast_path = ''\n",
    "        if not src.dim() == 3:\n",
    "            why_not_sparsity_fast_path = f\"input not batched; expected src.dim() of 3 but got {src.dim()}\"\n",
    "        elif self.training:\n",
    "            why_not_sparsity_fast_path = \"training is enabled\"\n",
    "        elif not self.self_attn.batch_first :\n",
    "            why_not_sparsity_fast_path = \"self_attn.batch_first was not True\"\n",
    "        elif not self.self_attn._qkv_same_embed_dim :\n",
    "            why_not_sparsity_fast_path = \"self_attn._qkv_same_embed_dim was not True\"\n",
    "        elif not self.activation_relu_or_gelu:\n",
    "            why_not_sparsity_fast_path = \"activation_relu_or_gelu was not True\"\n",
    "        elif not (self.norm1.eps == self.norm2.eps):\n",
    "            why_not_sparsity_fast_path = \"norm1.eps is not equal to norm2.eps\"\n",
    "        elif src_mask is not None:\n",
    "            why_not_sparsity_fast_path = \"src_mask is not supported for fastpath\"\n",
    "        elif src.is_nested and src_key_padding_mask is not None:\n",
    "            why_not_sparsity_fast_path = \"src_key_padding_mask is not supported with NestedTensor input for fastpath\"\n",
    "        elif self.self_attn.num_heads % 2 == 1:\n",
    "            why_not_sparsity_fast_path = \"num_head is odd\"\n",
    "        elif torch.is_autocast_enabled():\n",
    "            why_not_sparsity_fast_path = \"autocast is enabled\"\n",
    "\n",
    "        if not why_not_sparsity_fast_path:\n",
    "            tensor_args = (\n",
    "                src,\n",
    "                self.self_attn.in_proj_weight,\n",
    "                self.self_attn.in_proj_bias,\n",
    "                self.self_attn.out_proj.weight,\n",
    "                self.self_attn.out_proj.bias,\n",
    "#                 self.norm1.weight,\n",
    "#                 self.norm1.bias,\n",
    "#                 self.norm2.weight,\n",
    "#                 self.norm2.bias,\n",
    "                self.linear1.weight,\n",
    "                self.linear1.bias,\n",
    "                self.linear2.weight,\n",
    "                self.linear2.bias,\n",
    "            )\n",
    "\n",
    "            # We have to use list comprehensions below because TorchScript does not support\n",
    "            # generator expressions.\n",
    "            if torch.overrides.has_torch_function(tensor_args):\n",
    "                why_not_sparsity_fast_path = \"some Tensor argument has_torch_function\"\n",
    "            elif not all((x.is_cuda or 'cpu' in str(x.device)) for x in tensor_args):\n",
    "                why_not_sparsity_fast_path = \"some Tensor argument is neither CUDA nor CPU\"\n",
    "            elif torch.is_grad_enabled() and any(x.requires_grad for x in tensor_args):\n",
    "                why_not_sparsity_fast_path = (\"grad is enabled and at least one of query or the \"\n",
    "                                              \"input/output projection weights or biases requires_grad\")\n",
    "\n",
    "            if not why_not_sparsity_fast_path:\n",
    "                return torch._transformer_encoder_layer_fwd(\n",
    "                    src,\n",
    "                    self.self_attn.embed_dim,\n",
    "                    self.self_attn.num_heads,\n",
    "                    self.self_attn.in_proj_weight,\n",
    "                    self.self_attn.in_proj_bias,\n",
    "                    self.self_attn.out_proj.weight,\n",
    "                    self.self_attn.out_proj.bias,\n",
    "                    self.activation_relu_or_gelu == 2,\n",
    "                    self.norm_first,\n",
    "                    self.norm1.eps,\n",
    "                    self.norm1.weight,\n",
    "                    self.norm1.bias,\n",
    "                    self.norm2.weight,\n",
    "                    self.norm2.bias,\n",
    "                    self.linear1.weight,\n",
    "                    self.linear1.bias,\n",
    "                    self.linear2.weight,\n",
    "                    self.linear2.bias,\n",
    "                    # TODO: if src_mask and src_key_padding_mask merge to single 4-dim mask\n",
    "                    src_mask if src_mask is not None else src_key_padding_mask,\n",
    "                    1 if src_key_padding_mask is not None else\n",
    "                    0 if src_mask is not None else\n",
    "                    None,\n",
    "                )\n",
    "\n",
    "\n",
    "        x = src\n",
    "        if self.norm_first:\n",
    "            x = x + self._sa_block(x, src_mask, src_key_padding_mask)\n",
    "            x = x + self._ff_block(x)\n",
    "        else:\n",
    "            x = x + self._sa_block(x, src_mask, src_key_padding_mask)\n",
    "            x = x + self._ff_block(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    # self-attention block\n",
    "    def _sa_block(self, x: torch.Tensor,\n",
    "                  attn_mask: Optional[torch.Tensor], key_padding_mask: Optional[torch.Tensor]) -> torch.Tensor:\n",
    "        x = self.self_attn(x, x, x,\n",
    "                           attn_mask=attn_mask,\n",
    "                           key_padding_mask=key_padding_mask,\n",
    "                           need_weights=False)[0]\n",
    "        return self.dropout1(x)\n",
    "\n",
    "    # feed forward block\n",
    "    def _ff_block(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
    "        return self.dropout2(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identical to DenseFusion\n",
    "from pspnet import PSPNet\n",
    "#Identical to DenseFusion\n",
    "psp_models = {\n",
    "    'resnet18': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet18'),\n",
    "    'resnet34': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=512, deep_features_size=256, backend='resnet34'),\n",
    "    'resnet50': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet50'),\n",
    "    'resnet101': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet101'),\n",
    "    'resnet152': lambda: PSPNet(sizes=(1, 2, 3, 6), psp_size=2048, deep_features_size=1024, backend='resnet152')\n",
    "}\n",
    "#Identical to DenseFusion\n",
    "\n",
    "# New\n",
    "class ModifiedResnet(nn.Module):\n",
    "\n",
    "    def __init__(self, usegpu=True, large=False):\n",
    "        super(ModifiedResnet, self).__init__()\n",
    "        if large:\n",
    "            self.model = psp_models['resnet50'.lower()]()\n",
    "        else:\n",
    "            self.model = psp_models['resnet18'.lower()]()\n",
    "        self.model = nn.DataParallel(self.model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x\n",
    "\n",
    "class TransformationNet(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(TransformationNet, self).__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.in1 = nn.Linear(input_dim, 64)\n",
    "        self.bn_1 = nn.LayerNorm(64)\n",
    "        \n",
    "        self.in2 = nn.Linear(64, 128)\n",
    "        self.bn_2 = nn.LayerNorm(128)\n",
    "        \n",
    "        self.in3 = nn.Linear(128, 128)\n",
    "        self.bn_3 = nn.LayerNorm(128)\n",
    "        \n",
    "        self.in4 = nn.Linear(128, 256)\n",
    "        self.bn_4 = nn.LayerNorm(256)\n",
    "        \n",
    "        self.in5 = nn.Linear(256, 512)\n",
    "        self.bn_5 = nn.LayerNorm(512)\n",
    "\n",
    "        self.fc_1 = nn.Linear(512, 512)\n",
    "        self.bn_6 = nn.LayerNorm(512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.bn_7 = nn.LayerNorm(512)\n",
    "        self.fc_3 = nn.Linear(512, self.output_dim*self.output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        num_points = x.shape[1]\n",
    "        # x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_1(self.in1(x)))\n",
    "        x = F.relu(self.bn_2(self.in2(x)))\n",
    "        x = F.relu(self.bn_3(self.in3(x)))\n",
    "        x = F.relu(self.bn_4(self.in4(x)))\n",
    "        x = F.relu(self.bn_5(self.in5(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "        \n",
    "        x = nn.MaxPool1d(num_points)(x)\n",
    "        x = x.view(-1, 512)\n",
    "\n",
    "        x = F.relu(self.bn_6(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_7(self.fc_2(x)))\n",
    "        x = self.fc_3(x)\n",
    "\n",
    "        identity_matrix = torch.eye(self.output_dim)\n",
    "        if torch.cuda.is_available():\n",
    "            identity_matrix = identity_matrix.cuda()\n",
    "        x = x.view(-1, self.output_dim, self.output_dim) + identity_matrix\n",
    "        return x\n",
    "\n",
    "\n",
    "class BasePointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, point_dimension):\n",
    "        super(BasePointNet, self).__init__()\n",
    "        self.input_transform = TransformationNet(input_dim=point_dimension, output_dim=point_dimension)\n",
    "        self.feature_transform = TransformationNet(input_dim=64, output_dim=64)\n",
    "        \n",
    "        self.conv_1 = nn.Linear(point_dimension, 64)\n",
    "        self.conv_2 = nn.Linear(64, 64)\n",
    "        self.conv_3 = nn.Linear(64, 128)\n",
    "        # self.conv_4 = nn.Linear(128, 128)\n",
    "        self.conv_5 = nn.Linear(128, 256)\n",
    "        # self.conv_6 = nn.Linear(256, 256)\n",
    "        self.conv_7 = nn.Linear(256, 1024)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(64)\n",
    "        self.bn_2 = nn.LayerNorm(64)\n",
    "        self.bn_3 = nn.LayerNorm(128)\n",
    "        # self.bn_4 = nn.LayerNorm(128)\n",
    "        self.bn_5 = nn.LayerNorm(256)\n",
    "        # self.bn_6 = nn.LayerNorm(256)\n",
    "        self.bn_7 = nn.LayerNorm(1024)\n",
    "        \n",
    "\n",
    "    def forward(self, x, plot=False):\n",
    "        num_points = x.shape[1]\n",
    "        \n",
    "        # input_transform = self.input_transform(x) # T-Net tensor [batch, 3, 3]\n",
    "        # x = torch.bmm(x, input_transform) # Batch matrix-matrix product \n",
    "        # x = x.transpose(2, 1)\n",
    "        tnet_out=x.cpu().detach().numpy()\n",
    "        feature_transform = torch.zeros([1,2,2]).to(device)\n",
    "        x = F.relu(self.bn_1(self.conv_1(x)))\n",
    "        x = F.relu(self.bn_2(self.conv_2(x)))\n",
    "        # x = x.transpose(2, 1)\n",
    "\n",
    "        # feature_transform = self.feature_transform(x) # T-Net tensor [batch, 64, 64]\n",
    "        # x = torch.bmm(x, feature_transform)\n",
    "        x_perp = x\n",
    "        # x = x.transpose(2, 1)\n",
    "        x = F.relu(self.bn_3(self.conv_3(x)))\n",
    "        # x = F.relu(self.bn_4(self.conv_4(x)))\n",
    "        x = F.relu(self.bn_5(self.conv_5(x)))\n",
    "        # x = F.relu(self.bn_6(self.conv_6(x)))\n",
    "        x = F.relu(self.bn_7(self.conv_7(x)))\n",
    "        x = x.transpose(2, 1)\n",
    "        x_g, ix = nn.MaxPool1d(num_points, return_indices=True)(x)  # max-pooling\n",
    "        x_g = x_g.view(-1, 1024)  # global feature vector \n",
    "        \n",
    "        return x_g, x_perp, feature_transform, tnet_out, ix\n",
    "\n",
    "class SegmentaionPointNet(nn.Module):\n",
    "\n",
    "    def __init__(self, out_dim=128, dropout=0.3, point_dimension=6):\n",
    "        super(SegmentaionPointNet, self).__init__()\n",
    "        self.base_pointnet = BasePointNet(point_dimension=point_dimension)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(1088, 512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.fc_3 = nn.Linear(512, out_dim)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(512)\n",
    "        self.bn_2 = nn.LayerNorm(512)\n",
    "\n",
    "        # self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_g, x, feature_transform, tnet_out, ix_maxpool = self.base_pointnet(x)\n",
    "        x_g = torch.unsqueeze(x_g,1)\n",
    "        x_g = torch.tile(x_g,[1,x.shape[1],1])\n",
    "        x = torch.cat([x,x_g],-1)\n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_2(self.fc_2(x)))\n",
    "        # x = self.dropout_1(x)\n",
    "\n",
    "        return self.fc_3(x), feature_transform, tnet_out, ix_maxpool\n",
    "    \n",
    "class ClassificationPointNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, out_dim, dropout=0.3, point_dimension=6):\n",
    "        super(ClassificationPointNet, self).__init__()\n",
    "        self.base_pointnet = BasePointNet(point_dimension=point_dimension)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(1024, 512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.fc_3 = nn.Linear(512, out_dim)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(512)\n",
    "        self.bn_2 = nn.LayerNorm(512)\n",
    "\n",
    "        # self.dropout_1 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _, feature_transform, tnet_out, ix_maxpool = self.base_pointnet(x)\n",
    "\n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_2(self.fc_2(x)))\n",
    "        # x = self.dropout_1(x)\n",
    "\n",
    "        return self.fc_3(x), feature_transform, tnet_out, ix_maxpool\n",
    "    \n",
    "class PointCloud_Matching(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PointCloud_Matching, self).__init__()\n",
    "        self.source_point_net = ClassificationPointNet(512)\n",
    "        self.target_point_net = ClassificationPointNet(512)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(1024, 512)\n",
    "        self.fc_2 = nn.Linear(512, 512)\n",
    "        self.fc_3 = nn.Linear(512, 3)\n",
    "        self.fc_4 = nn.Linear(512, 3)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(512)\n",
    "        self.bn_2 = nn.LayerNorm(512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        source_pcs, target_pcs = x\n",
    "        \n",
    "        s_x, s_ft, s_tn, _ = self.source_point_net(source_pcs)\n",
    "        t_x, t_ft, t_tn, _ = self.target_point_net(target_pcs)\n",
    "        \n",
    "        x = torch.cat([s_x,t_x],dim=-1)\n",
    "        \n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        x = F.relu(self.bn_2(self.fc_2(x)))\n",
    "        \n",
    "        R = torch.tanh(self.fc_3(x)) * np.pi\n",
    "        t = self.fc_4(x)\n",
    "        \n",
    "        return R, t, s_ft, t_ft\n",
    "    \n",
    "class PointCloud_Matching_pixel_wise(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PointCloud_Matching_pixel_wise, self).__init__()\n",
    "        self.source_point_net = SegmentaionPointNet(128,point_dimension=6)\n",
    "        self.target_point_net = ClassificationPointNet(64,point_dimension=6)\n",
    "        self.conv = ModifiedResnet()\n",
    "        \n",
    "        self.fc_1 = nn.Linear(128+64, 32)\n",
    "        self.fc_2 = nn.Linear(64, 256)\n",
    "        self.fc_3 = nn.Linear(256, 32)\n",
    "\n",
    "        self.bn_1 = nn.LayerNorm(32)\n",
    "        self.bn_2 = nn.LayerNorm(256)\n",
    "        self.bn_3 = nn.LayerNorm(32)\n",
    "        \n",
    "        self.r_fc_1 = nn.Linear(3*32, 512)\n",
    "        self.r_bn_1 = nn.LayerNorm(512)\n",
    "        self.r_fc_2 = nn.Linear(512, 512)\n",
    "        self.r_bn_2 = nn.LayerNorm(512)\n",
    "        self.r_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        \n",
    "        self.t_fc_1 = nn.Linear(3*32, 512)\n",
    "        self.t_bn_1 = nn.LayerNorm(512)\n",
    "        self.t_fc_2 = nn.Linear(512, 512)\n",
    "        self.t_bn_2 = nn.LayerNorm(512)\n",
    "        self.t_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        self.c_fc_1 = nn.Linear(3*32, 512)\n",
    "        self.c_bn_1 = nn.LayerNorm(512)\n",
    "        self.c_fc_2 = nn.Linear(512, 512)\n",
    "        self.c_bn_2 = nn.LayerNorm(512)\n",
    "        self.c_fc_3 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        source_pcs, target_pcs, rgb, mask = x\n",
    "        \n",
    "        per_pixel = self.conv(rgb)\n",
    "        per_pixel = torch.unsqueeze(torch.permute(per_pixel,[0,2,3,1])[0][mask],0)\n",
    "        \n",
    "        s_x, s_ft, s_tn, _ = self.source_point_net(source_pcs)\n",
    "        t_x, t_ft, t_tn, _ = self.target_point_net(target_pcs)\n",
    "        \n",
    "        t_x = torch.unsqueeze(t_x,1)\n",
    "        t_x = torch.tile(t_x,[1,s_x.shape[1],1])\n",
    "        \n",
    "        x = torch.cat([s_x,t_x],dim=-1)\n",
    "        x = F.relu(self.bn_1(self.fc_1(x)))\n",
    "        \n",
    "        x_perp = torch.cat([x,per_pixel],-1)\n",
    "        \n",
    "        x = F.relu(self.bn_2(self.fc_2(x_perp)))\n",
    "        x = F.relu(self.bn_3(self.fc_3(x)))\n",
    "        \n",
    "        num_points = x.shape[1]\n",
    "        x_g = nn.AvgPool1d(num_points)(x.transpose(2, 1)).transpose(2, 1)\n",
    "        \n",
    "        # x_g = torch.unsqueeze(x_g,1)\n",
    "        x_g = torch.tile(x_g,[1,x_perp.shape[1],1])\n",
    "        \n",
    "        x_final = torch.cat([x_perp,x_g],-1)\n",
    "        \n",
    "        R =  F.relu(self.r_bn_1(self.r_fc_1(x_final)))\n",
    "        R =  F.relu(self.r_bn_2(self.r_fc_2(R)))\n",
    "        R =  torch.tanh(self.r_fc_3(R)) * np.pi\n",
    "        \n",
    "        t =  F.relu(self.t_bn_1(self.t_fc_1(x_final)))\n",
    "        t =  F.relu(self.t_bn_2(self.t_fc_2(t)))\n",
    "        t =  self.t_fc_3(t)\n",
    "        \n",
    "        c =  F.relu(self.c_bn_1(self.c_fc_1(x_final)))\n",
    "        c =  F.relu(self.c_bn_2(self.c_fc_2(c)))\n",
    "        c =  self.c_fc_3(c)\n",
    "        \n",
    "        return R, t, c, s_ft, t_ft\n",
    "\n",
    "class adaptive_layer_norm(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, condition_size):\n",
    "        super(adaptive_layer_norm, self).__init__()\n",
    "        \n",
    "        self.norm = nn.LayerNorm(channels)\n",
    "        \n",
    "        self.beta = nn.Linear(condition_size,channels)\n",
    "        self.gamma = nn.Linear(condition_size,channels)\n",
    "     \n",
    "    def forward(self, x, c):\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        gamma = self.gamma(c)\n",
    "        beta = self.beta(c)\n",
    "        \n",
    "        x = x + x * gamma + beta\n",
    "        \n",
    "        return x\n",
    "\n",
    "class PointCloud_Matching_Graph(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PointCloud_Matching_Graph, self).__init__()\n",
    "        self.source_gnn = EncoderProcessorDecoder(6,1,128,128,2,True,5,128)\n",
    "        self.target_gnn = EncoderProcessorDecoder(6,1,128,128,2,True,5,128)\n",
    "        \n",
    "        self.fc_1 = nn.Linear(128, 256)\n",
    "        self.fc_2 = nn.Linear(256, 512)\n",
    "        self.fc_3 = nn.Linear(512, 512)\n",
    "        \n",
    "        self.r_fc_1 = nn.Linear(512, 512)\n",
    "        self.r_fc_2 = nn.Linear(512, 512)\n",
    "        self.r_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        self.t_fc_1 = nn.Linear(512, 512)\n",
    "        self.t_fc_2 = nn.Linear(512, 512)\n",
    "        self.t_fc_3 = nn.Linear(512, 3)\n",
    "        \n",
    "        self.aln1 = adaptive_layer_norm(256,128)\n",
    "        self.aln2 = adaptive_layer_norm(512,128)\n",
    "        self.aln3 = adaptive_layer_norm(512,128)\n",
    "        \n",
    "        self.raln1 = adaptive_layer_norm(512,128)\n",
    "        self.raln2 = adaptive_layer_norm(512,128)\n",
    "        \n",
    "        self.taln1 = adaptive_layer_norm(512,128)\n",
    "        self.taln2 = adaptive_layer_norm(512,128)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        source_graph, target_graph, s_b, t_b = x\n",
    "        \n",
    "        \n",
    "        s_x = self.source_gnn(source_graph)\n",
    "        t_x = self.target_gnn(target_graph)\n",
    "        \n",
    "        s_x = torch_geometric.nn.global_max_pool(s_x,s_b)\n",
    "        t_x = torch_geometric.nn.global_max_pool(t_x,t_b)\n",
    "        \n",
    "        x = F.elu(self.aln1(self.fc_1(s_x),t_x))\n",
    "        x = F.elu(self.aln2(self.fc_2(x),t_x))\n",
    "        x = F.elu(self.aln3(self.fc_3(x),t_x))\n",
    "        \n",
    "        R =  F.elu(self.raln1(self.r_fc_1(x),t_x))\n",
    "        R =  F.elu(self.raln2(self.r_fc_2(R),t_x))\n",
    "        R =  torch.tanh(self.r_fc_3(R)) * np.pi\n",
    "        \n",
    "        t =  F.elu(self.taln1(self.t_fc_1(x),t_x))\n",
    "        t =  F.elu(self.taln2(self.t_fc_2(t),t_x))\n",
    "        t =  self.t_fc_3(t)\n",
    "        \n",
    "        return R, t\n",
    "    \n",
    "class PoseNetFeat(nn.Module):\n",
    "    def __init__(self, large = False):\n",
    "        super(PoseNetFeat, self).__init__()\n",
    "        if large: \n",
    "            self.conv1 = torch.nn.Conv1d(3, 256, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "            self.e_conv1 = torch.nn.Conv1d(32, 256, 1)\n",
    "            self.e_conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "        else:\n",
    "            self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "            self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "            self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(256, 512, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        \n",
    "        num_points = x.shape[2]\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        emb = F.relu(self.e_conv1(emb))\n",
    "        pointfeat_1 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        emb = F.relu(self.e_conv2(emb))\n",
    "        pointfeat_2 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv5(pointfeat_2))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        ap_x = torch.nn.AvgPool1d(num_points)(x)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024, 1).repeat(1, 1, num_points)\n",
    "        return torch.cat([pointfeat_1, pointfeat_2, ap_x], 1) #128 + 256 + 1024\n",
    "\n",
    "class PoseNet(nn.Module):\n",
    "    def __init__(self, num_obj, large=False):\n",
    "        super(PoseNet, self).__init__()\n",
    "        # self.num_points = num_points\n",
    "        self.cnn = ModifiedResnet().to(device)\n",
    "        self.feat = PoseNetFeat(large=large)\n",
    "        if large:\n",
    "            self.conv1_r = torch.nn.Conv1d(2048, 1024, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(2048, 1024, 1)\n",
    "            self.conv1_c = torch.nn.Conv1d(2048, 1024, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(1024, 512, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(1024, 512, 1)\n",
    "            self.conv2_c = torch.nn.Conv1d(1024, 512, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv3_c = torch.nn.Conv1d(512, 512, 1)\n",
    "\n",
    "            self.conv4_r = torch.nn.Conv1d(512, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(512, num_obj*3, 1) #translation\n",
    "            self.conv4_c = torch.nn.Conv1d(512, num_obj*1, 1) #confidence\n",
    "        else:\n",
    "            self.conv1_r = torch.nn.Conv1d(1408, 640, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(1408, 640, 1)\n",
    "            self.conv1_c = torch.nn.Conv1d(1408, 640, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(640, 256, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(640, 256, 1)\n",
    "            self.conv2_c = torch.nn.Conv1d(640, 256, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(256, 128, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(256, 128, 1)\n",
    "            self.conv3_c = torch.nn.Conv1d(256, 128, 1)\n",
    "\n",
    "            self.conv4_r = torch.nn.Conv1d(128, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(128, num_obj*3, 1) #translation\n",
    "            self.conv4_c = torch.nn.Conv1d(128, num_obj*1, 1) #confidence\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, img, x, mask, obj):\n",
    "        out_img = self.cnn(img)\n",
    "        num_points = x.shape[1]\n",
    "        out_img = torch.permute(out_img[0],[1,2,0])\n",
    "        emb = torch.unsqueeze(out_img[mask],0).transpose(2,1).contiguous()\n",
    "        \n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        ap_x = self.feat(x, emb)\n",
    "\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))\n",
    "        cx = F.relu(self.conv1_c(ap_x))      \n",
    "\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "        cx = F.relu(self.conv2_c(cx))\n",
    "\n",
    "        rx = F.relu(self.conv3_r(rx))\n",
    "        tx = F.relu(self.conv3_t(tx))\n",
    "        cx = F.relu(self.conv3_c(cx))\n",
    "\n",
    "        rx = self.conv4_r(rx).view(1, self.num_obj, 4, num_points)\n",
    "        tx = self.conv4_t(tx).view(1, self.num_obj, 3, num_points)\n",
    "        cx = self.conv4_c(cx).view(1, self.num_obj, 1, num_points)\n",
    "        \n",
    "        out_rx = rx[:,obj,:,:]\n",
    "        out_tx = tx[:,obj,:,:]\n",
    "        out_cx = cx[:,obj,:,:]\n",
    "        \n",
    "        out_rx = out_rx.contiguous().transpose(2, 1).contiguous()\n",
    "        out_cx = out_cx.contiguous().transpose(2, 1).contiguous()\n",
    "        out_tx = out_tx.contiguous().transpose(2, 1).contiguous()\n",
    "        \n",
    "        return out_rx, out_tx, out_cx, emb.detach()\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of SelfAttentionPooling \n",
    "    Original Paper: Self-Attention Encoding and Pooling for Speaker Recognition\n",
    "    https://arxiv.org/pdf/2008.01077v1.pdf\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim):\n",
    "        super(AttentionPooling, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, batch_rep):\n",
    "        \"\"\"\n",
    "        input:\n",
    "            batch_rep : size (N, T, H), N: batch size, T: sequence length, H: Hidden dimension\n",
    "        \n",
    "        attention_weight:\n",
    "            att_w : size (N, T, 1)\n",
    "        \n",
    "        return:\n",
    "            utter_rep: size (N, H)\n",
    "        \"\"\"\n",
    "        softmax = nn.functional.softmax\n",
    "        att_w = softmax(self.W(batch_rep).squeeze(-1)).unsqueeze(-1)\n",
    "        utter_rep = torch.sum(batch_rep * att_w, dim=1)\n",
    "\n",
    "        return utter_rep\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "class PoseNet_Attn(nn.Module):\n",
    "    def __init__(self, num_obj, large=False):\n",
    "        super(PoseNet_Attn, self).__init__()\n",
    "        # self.num_points = num_points\n",
    "        self.cnn = ModifiedResnet().to(device)\n",
    "        self.feat = PoseNetFeat(large=large)\n",
    "        if large:\n",
    "            self.conv1_r = torch.nn.Conv1d(2048, 1024, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(2048, 1024, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(1024, 512, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(1024, 512, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(512, 512, 1)\n",
    "            \n",
    "            self.r_attn_pool = AttentionPooling(512)\n",
    "            self.t_attn_pool = AttentionPooling(512)\n",
    "            \n",
    "            self.conv4_r = torch.nn.Conv1d(512, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(512, num_obj*3, 1) #translation\n",
    "        else:\n",
    "            \n",
    "#             self.lrbl_e_r = torch.nn.Parameter(torch.randn(128))\n",
    "#             self.lrbl_e_r.requires_grad = True\n",
    "            \n",
    "#             self.lrbl_e_t = torch.nn.Parameter(torch.randn(128))\n",
    "#             self.lrbl_e_t.requires_grad = True\n",
    "            \n",
    "#             self.TEL_r = TransformerEncoderLayer_NoNorm(128,8)\n",
    "#             self.transf_r = nn.TransformerEncoder(self.TEL_r, num_layers=6)\n",
    "            \n",
    "#             self.TEL_t = TransformerEncoderLayer_NoNorm(128,8)\n",
    "#             self.transf_t = nn.TransformerEncoder(self.TEL_t, num_layers=6)\n",
    "            \n",
    "            self.conv1_r = torch.nn.Conv1d(1408, 640, 1)\n",
    "            self.conv1_t = torch.nn.Conv1d(1408, 640, 1)\n",
    "\n",
    "            self.conv2_r = torch.nn.Conv1d(640, 256, 1)\n",
    "            self.conv2_t = torch.nn.Conv1d(640, 256, 1)\n",
    "\n",
    "            self.conv3_r = torch.nn.Conv1d(256, 128, 1)\n",
    "            self.conv3_t = torch.nn.Conv1d(256, 128, 1)\n",
    "            \n",
    "            self.r_attn_pool = AttentionPooling(128)\n",
    "            self.t_attn_pool = AttentionPooling(128)\n",
    "\n",
    "            self.conv4_r = torch.nn.Conv1d(128, num_obj*4, 1) #quaternion\n",
    "            self.conv4_t = torch.nn.Conv1d(128, num_obj*3, 1) #translation\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, img, x, mask, obj):\n",
    "        out_img = self.cnn(img)\n",
    "        num_points = x.shape[1]\n",
    "        out_img = torch.permute(out_img[0],[1,2,0])\n",
    "        emb = torch.unsqueeze(out_img[mask],0).transpose(2,1).contiguous()\n",
    "        bs = x.shape[0]\n",
    "        \n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        ap_x = self.feat(x, emb)\n",
    "\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))\n",
    "\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "\n",
    "        rx = F.relu(self.conv3_r(rx))\n",
    "        tx = F.relu(self.conv3_t(tx))\n",
    "        \n",
    "        rx = self.r_attn_pool(rx.transpose(2,1)).unsqueeze(-1)\n",
    "        tx = self.t_attn_pool(tx.transpose(2,1)).unsqueeze(-1)\n",
    "\n",
    "#         rx = self.transf_r(torch.cat([self.lrbl_e_r.unsqueeze(0).unsqueeze(0).repeat([bs,1,1]), rx.transpose(2,1)],1))[:,0:1,:].transpose(2,1)\n",
    "#         tx = self.transf_t(torch.cat([self.lrbl_e_t.unsqueeze(0).unsqueeze(0).repeat([bs,1,1]), tx.transpose(2,1)],1))[:,0:1,:].transpose(2,1)\n",
    "\n",
    "        rx = self.conv4_r(rx).view(bs, self.num_obj, 4)\n",
    "        tx = self.conv4_t(tx).view(bs, self.num_obj, 3)\n",
    "        \n",
    "        out_rx = rx[:,obj:obj+1,:]\n",
    "        out_tx = tx[:,obj:obj+1,:]\n",
    "        \n",
    "        \n",
    "        return out_rx, out_tx, None, emb.detach()\n",
    "    \n",
    "class PoseNet_Encoder(nn.Module):\n",
    "    def __init__(self, large = False):\n",
    "        super(PoseNet_Encoder, self).__init__()\n",
    "        if large: \n",
    "            self.conv1 = torch.nn.Conv1d(3, 256, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "#             self.e_conv1 = torch.nn.Conv1d(32, 256, 1)\n",
    "#             self.e_conv2 = torch.nn.Conv1d(256, 256, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(512, 512, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "        else:\n",
    "            self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "            self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "#             self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "#             self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "            self.conv5 = torch.nn.Conv1d(128, 128, 1)\n",
    "            self.conv6 = torch.nn.Conv1d(128, 128, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        num_points = x.shape[2]\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "#         emb = F.relu(self.e_conv1(emb))\n",
    "#         pointfeat_1 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "#         emb = F.relu(self.e_conv2(emb))\n",
    "#         pointfeat_2 = torch.cat((x, emb), dim=1)\n",
    "\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        ap_x = torch.nn.AvgPool1d(num_points)(x)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 128, 1)\n",
    "        return ap_x    \n",
    "\n",
    "class PoseRefineNetFeat(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoseRefineNetFeat, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n",
    "        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "\n",
    "        self.e_conv1 = torch.nn.Conv1d(32, 64, 1)\n",
    "        self.e_conv2 = torch.nn.Conv1d(64, 128, 1)\n",
    "        \n",
    "        self.ref_conv1 = torch.nn.Conv1d(128, 128, 1)\n",
    "        self.ref_conv2 = torch.nn.Conv1d(128, 128, 1)\n",
    "\n",
    "        self.conv5 = torch.nn.Conv1d(256+128, 512, 1)\n",
    "        self.conv6 = torch.nn.Conv1d(512, 1024, 1)\n",
    "\n",
    "    def forward(self, x, emb):\n",
    "        num_points = x.shape[2]\n",
    "        x = F.relu(self.conv1(x))\n",
    "        emb = F.relu(self.e_conv1(emb))\n",
    "#         ref_e = F.relu(self.ref_conv1(ref_e)).repeat(1, 1, num_points)\n",
    "        pointfeat_1 = torch.cat([x, emb], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv2(x))\n",
    "        emb = F.relu(self.e_conv2(emb))\n",
    "#         ref_e = F.relu(self.ref_conv2(ref_e))\n",
    "        pointfeat_2 = torch.cat([x, emb], dim=1)\n",
    "\n",
    "        pointfeat_3 = torch.cat([pointfeat_1, pointfeat_2], dim=1)\n",
    "\n",
    "        x = F.relu(self.conv5(pointfeat_3))\n",
    "        x = F.relu(self.conv6(x))\n",
    "\n",
    "        ap_x = torch.nn.AvgPool1d(num_points)(x)\n",
    "\n",
    "        ap_x = ap_x.view(-1, 1024)\n",
    "        return ap_x\n",
    "\n",
    "class PoseRefineNet(nn.Module):\n",
    "    def __init__(self, num_obj):\n",
    "        super(PoseRefineNet, self).__init__()\n",
    "        self.feat = PoseRefineNetFeat()\n",
    "#         self.ref_enc = PoseNet_Encoder()\n",
    "        \n",
    "        self.conv1_r = torch.nn.Linear(1024, 512)\n",
    "        self.conv1_t = torch.nn.Linear(1024, 512)\n",
    "\n",
    "        self.conv2_r = torch.nn.Linear(512, 128)\n",
    "        self.conv2_t = torch.nn.Linear(512, 128)\n",
    "\n",
    "        self.conv3_r = torch.nn.Linear(128, num_obj*4) #quaternion\n",
    "        self.conv3_t = torch.nn.Linear(128, num_obj*3) #translation\n",
    "\n",
    "        self.num_obj = num_obj\n",
    "\n",
    "    def forward(self, x, emb, obj):\n",
    "        \n",
    "        num_points = x.shape[1]\n",
    "        bs = x.shape[0]\n",
    "#         ref_e = self.ref_enc(ref_x.transpose(2, 1).contiguous())\n",
    "        x = x.transpose(2, 1).contiguous()\n",
    "        ap_x = self.feat(x, emb)\n",
    "\n",
    "        rx = F.relu(self.conv1_r(ap_x))\n",
    "        tx = F.relu(self.conv1_t(ap_x))   \n",
    "\n",
    "        rx = F.relu(self.conv2_r(rx))\n",
    "        tx = F.relu(self.conv2_t(tx))\n",
    "\n",
    "        rx = self.conv3_r(rx).view(bs, self.num_obj, 4)\n",
    "        tx = self.conv3_t(tx).view(bs, self.num_obj, 3)\n",
    "\n",
    "        out_rx = rx[:,obj,:]\n",
    "        out_tx = tx[:,obj,:]\n",
    "\n",
    "        return out_rx, out_tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b888a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_posnet(i, aug=True):\n",
    "    sym = [1,2,10,13,14]\n",
    "    pc,c,obj_id,R,t = point_cloud_pairs[i]\n",
    "    idx = idx_list[i]\n",
    "    rgb = rgbs[idx]/255.0\n",
    "    depth = np.zeros(rgb.shape)\n",
    "    mask = (merged_masks[idx] == obj_to_mask_value[obj_id-1])\n",
    "    # rgb[np.logical_not(mask)] = 0.0\n",
    "    depth = depth_imgs[idx]\n",
    "    \n",
    "    if aug:\n",
    "        angle = 0.0\n",
    "    else:\n",
    "        angle = 0.0\n",
    "    rgb = np.transpose(torchvision.transforms.functional.rotate(torch.Tensor(np.transpose(rgb,[2,0,1])),angle).detach().numpy(),[1,2,0])\n",
    "    mask = torchvision.transforms.functional.rotate(torch.Tensor(np.transpose(np.expand_dims(mask,-1),[2,0,1]).astype(np.float32)),angle).detach().numpy()[0].astype(bool)\n",
    "    depth = np.transpose(torchvision.transforms.functional.rotate(torch.Tensor(np.transpose(depth,[2,0,1])),angle).detach().numpy(),[1,2,0])\n",
    "    \n",
    "    if mask.sum()>500:\n",
    "        ridx = np.random.choice(mask.sum(),size=mask.sum()-500,replace=False)\n",
    "        i1,i2 = np.where(mask)\n",
    "        mask[i1[ridx],i2[ridx]] = False\n",
    "    \n",
    "    pc = depth[mask]\n",
    "    obj_class = obj_id - 1\n",
    "    target_pc = reference_point_clouds[obj_id - 1][0]\n",
    "    \n",
    "    RR = roma.rotvec_to_rotmat(torch.Tensor([0,0,-angle*np.pi/180])).detach().numpy()\n",
    "    pc = (RR@pc.T).T\n",
    "    \n",
    "    if target_pc.shape[0] > 500:\n",
    "        target_pc = target_pc[np.random.choice(target_pc.shape[0],size=500, replace=False)]\n",
    "        \n",
    "    target = (RR@((np.reshape(R,[3,3])@target_pc.T).T + t).T).T\n",
    "    \n",
    "    if aug:\n",
    "        pass\n",
    "        rgb = rgb + np.random.uniform(low =-0.3, high=0.3,size=rgb.shape)\n",
    "        rgb = np.minimum(rgb,1.0)\n",
    "        rgb = np.maximum(rgb,0.0)\n",
    "        # pc = pc + np.random.uniform(low =-0.002, high=0.002,size=pc.shape)\n",
    "    \n",
    "    return rgb, pc*1000.0 ,mask,obj_class, R, t, obj_id in sym, diameters[obj_id-1], target*1000.0, target_pc*1000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13668960",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_model = PoseNet_Attn(18).to(device)\n",
    "\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "optimizer = torch.optim.Adam(pose_model.parameters(), lr=learning_rate)\n",
    "decay_stepper = torch.optim.lr_scheduler.StepLR(optimizer, 5, gamma=0.6)\n",
    "\n",
    "w = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e003e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_cloud_pairs_train = []\n",
    "point_cloud_pairs_test = []\n",
    "for i,pcp in enumerate(point_cloud_pairs):\n",
    "    if idx_list[i] in train:\n",
    "        point_cloud_pairs_train.append(i)\n",
    "    elif idx_list[i] in test:\n",
    "        point_cloud_pairs_test.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f8bd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ADD = 0.0\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    pose_model.eval()\n",
    "    if epoch>0:\n",
    "        for i in tqdm(point_cloud_pairs_test):\n",
    "            cc+= 1\n",
    "\n",
    "            rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i,aug=False)\n",
    "\n",
    "            if pc.shape[0]>0:\n",
    "                R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "                t = t\n",
    "                R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "                Rs = roma.unitquat_to_rotmat(R[:,0,:])\n",
    "                t = t[:,0,:]\n",
    "                transformed = torch.bmm(Rs,torch.Tensor(np.expand_dims(target_pc,0)).transpose(1,2).to(device)).transpose(1,2) + t\n",
    "                target = torch.Tensor(target).to(device)\n",
    "\n",
    "                if is_sym:\n",
    "                    L_p = torch.mean(torch.min(torch.cdist(transformed,torch.unsqueeze(target,0)),dim=-1)[0])\n",
    "                else:\n",
    "                    L_p = torch.mean(torch.norm(transformed[0] - target,dim=-1),dim=-1)\n",
    "\n",
    "                if L_p <= 0.1 * d:\n",
    "                    ADD += 1\n",
    "\n",
    "                l_ov += L_p.cpu().detach().numpy()\n",
    "        if ADD >= best_ADD:\n",
    "            best_ADD = ADD\n",
    "            if epoch > 0:\n",
    "                torch.save(pose_model,'best_custom_new.pt')\n",
    "        print('Validation Lp: %.7f, ADD(-S): %.7f'%(l_ov/cc,ADD/cc))\n",
    "    \n",
    "    shuffle = np.random.choice(int(point_cloud_pairs.shape[0] * 0.9), size = int(point_cloud_pairs.shape[0] * 0.9),replace=False)\n",
    "    random.shuffle(point_cloud_pairs_train)\n",
    "    prog = tqdm(point_cloud_pairs_train)\n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    for i in prog:\n",
    "        cc+= 1\n",
    "        if epoch == 0 and cc<=1000:\n",
    "            optimizer.param_groups[0]['lr'] = 1e-5 * 10**(cc/1000.)\n",
    "        optimizer.zero_grad()\n",
    "        pose_model = pose_model.train()\n",
    "        \n",
    "        rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i)\n",
    "        if pc.shape[0]>0:\n",
    "            R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "            t = t\n",
    "            R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "            Rs = roma.unitquat_to_rotmat(R)\n",
    "            transformed = torch.bmm(Rs[0],torch.tile(torch.Tensor(np.expand_dims(target_pc,0)),[Rs.shape[1],1,1]).transpose(1,2).to(device)).transpose(1,2) + torch.unsqueeze(t[0],1)\n",
    "            target = torch.Tensor(target).to(device)\n",
    "            \n",
    "            if is_sym:\n",
    "                L_p = torch.cdist(transformed.reshape(-1,3),target).min(-1)[0].reshape(transformed.shape[0],transformed.shape[1]).mean(-1)\n",
    "                \n",
    "            else:\n",
    "                target = torch.unsqueeze(target,0)\n",
    "                \n",
    "                L_p = torch.mean(torch.norm(transformed - target,dim=-1),dim=-1)\n",
    "\n",
    "            if L_p[0].cpu().detach().numpy() <= 0.1 * d:\n",
    "                ADD += 1\n",
    "            # print(c)\n",
    "            loss = torch.mean(L_p)\n",
    "            # loss = torch.mean(L_p)\n",
    "            l_ov += torch.min(L_p).cpu().detach().numpy()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            prog.set_postfix_str('ADD(-S): %.7f, overall_loss: %.7f, loss: %.7f, Lp: %.7f'%(ADD/cc,l_ov/cc,loss.cpu().detach().numpy(),torch.min(L_p).cpu().detach().numpy()))\n",
    "        \n",
    "    decay_stepper.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ebb7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_model = PoseRefineNet(18).to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 100\n",
    "\n",
    "optimizer_r = torch.optim.Adam(refine_model.parameters(), lr=learning_rate)\n",
    "decay_stepper_r = torch.optim.lr_scheduler.StepLR(optimizer_r, 10, gamma=0.6)\n",
    "\n",
    "pose_model = pose_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bc1f3b-3659-4e9c-9fc7-ee1b5a8fba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ADD = 0.75\n",
    "for epoch in range(epochs):\n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    refine_model = refine_model.eval()\n",
    "    if epoch>0:\n",
    "        for i in tqdm(point_cloud_pairs_test):\n",
    "            cc+= 1\n",
    "\n",
    "            rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i,aug=False)\n",
    "            if pc.shape[0]>0:\n",
    "                torch_rgb = torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device)\n",
    "                torch_pc = torch.Tensor(np.expand_dims(pc,0)).to(device)\n",
    "                torch_target = torch.Tensor(target).to(device)\n",
    "\n",
    "                R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "                t = t\n",
    "                R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "                Rs = roma.unitquat_to_rotmat(R)[:,0]\n",
    "                Rs_i = roma.rotmat_inverse(Rs)\n",
    "\n",
    "                new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - t).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "                R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "                R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "                R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "                R_final = torch.bmm(R_r,Rs)\n",
    "                T_final = T_r + t[:,0]\n",
    "                \n",
    "                Rs_i = roma.rotmat_inverse(R_final)\n",
    "                new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "                R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "                R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "                R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "                R_final = torch.bmm(R_r,R_final)\n",
    "                T_final += T_r\n",
    "\n",
    "                Rs_i = roma.rotmat_inverse(R_final)\n",
    "                new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "                R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "                R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "                R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "                R_final = torch.bmm(R_r,R_final)\n",
    "                T_final += T_r\n",
    "                \n",
    "                transformed = torch.bmm(R_final, torch.Tensor(target_pc.T).to(device).unsqueeze(0).repeat([R_final.shape[0],1,1])).transpose(1,2) + T_final.unsqueeze(1)\n",
    "\n",
    "                if is_sym:\n",
    "                    L_p = torch.cdist(transformed.reshape(-1,3),torch_target).min(-1)[0].reshape(transformed.shape[0],transformed.shape[1]).mean(-1)\n",
    "\n",
    "                else:\n",
    "                    L_p = torch.norm(transformed - torch_target.unsqueeze(0),dim=-1).mean(-1)\n",
    "\n",
    "                if L_p <= 0.1 * d:\n",
    "                        ADD += 1\n",
    "\n",
    "                l_ov += L_p.cpu().detach().numpy()\n",
    "            \n",
    "        if ADD >= best_ADD:\n",
    "            best_ADD = ADD\n",
    "            if epoch > 0:\n",
    "                torch.save(refine_model,'best_linemode_refine_final.pt')\n",
    "        print('Validation Lp: %.7f, ADD(-S): %.7f'%(l_ov/cc,ADD/cc))\n",
    "    \n",
    "    random.shuffle(point_cloud_pairs_train)\n",
    "    prog = tqdm(point_cloud_pairs_train)\n",
    "    l_ov = 0.0\n",
    "    ADD = 0\n",
    "    cc = 0\n",
    "    \n",
    "    for i in prog:\n",
    "        cc+= 1\n",
    "        \n",
    "        optimizer_r.zero_grad()\n",
    "        refine_model = refine_model.train()\n",
    "        \n",
    "        rgb,pc,mask,obj_class, _, _, is_sym, d, target, target_pc = get_sample_posnet(i)\n",
    "        if pc.shape[0]>0:\n",
    "            torch_rgb = torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device)\n",
    "            torch_pc = torch.Tensor(np.expand_dims(pc,0)).to(device)\n",
    "            torch_target = torch.Tensor(target).to(device)\n",
    "\n",
    "            R,t,c,emb = pose_model(torch.Tensor(np.expand_dims(np.transpose(rgb,[2,0,1]),0)).to(device),torch.Tensor(np.expand_dims(pc,0)).to(device),mask,obj_class)\n",
    "            t = t\n",
    "            R = R / (torch.norm(R, dim=2).view(1, -1, 1))\n",
    "            Rs = roma.unitquat_to_rotmat(R)[:,0]\n",
    "            Rs_i = roma.rotmat_inverse(Rs)\n",
    "\n",
    "            new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - t).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "            R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "            R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "            R_r = roma.unitquat_to_rotmat(R_r)\n",
    "        \n",
    "            R_final = torch.bmm(R_r,Rs)\n",
    "            T_final = T_r + t[:,0]\n",
    "            \n",
    "            Rs_i = roma.rotmat_inverse(R_final)\n",
    "            new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "            R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "            R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "            R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "            R_final = torch.bmm(R_r,R_final)\n",
    "            T_final += T_r\n",
    "\n",
    "            Rs_i = roma.rotmat_inverse(R_final)\n",
    "            new_pc = torch.bmm(Rs_i,(torch_pc.repeat(Rs_i.shape[0],1,1) - T_final.unsqueeze(1)).transpose(1,2)).transpose(1,2)\n",
    "\n",
    "            R_r,T_r = refine_model(new_pc,emb.repeat(new_pc.shape[0],1,1),obj_class)\n",
    "\n",
    "            R_r = R_r / (torch.norm(R_r, dim=-1).view(-1, 1))\n",
    "            R_r = roma.unitquat_to_rotmat(R_r)\n",
    "\n",
    "            R_final = torch.bmm(R_r,R_final)\n",
    "            T_final += T_r\n",
    "            \n",
    "            transformed = torch.bmm(R_final, torch.Tensor(target_pc.T).to(device).unsqueeze(0).repeat([R_final.shape[0],1,1])).transpose(1,2) + T_final.unsqueeze(1)\n",
    "            \n",
    "            if is_sym:\n",
    "                L_p = torch.cdist(transformed.reshape(-1,3),torch_target).min(-1)[0].reshape(transformed.shape[0],transformed.shape[1]).mean(-1)\n",
    "\n",
    "            else:\n",
    "                L_p = torch.norm(transformed - torch_target.unsqueeze(0),dim=-1).mean(-1)\n",
    "                \n",
    "            if L_p <= 0.1 * d:\n",
    "                    ADD += 1\n",
    "            \n",
    "            l_ov += L_p.cpu().detach().numpy()\n",
    "                \n",
    "            loss = L_p.mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer_r.step()\n",
    "\n",
    "            prog.set_postfix_str('ADD(-S): %.7f, overall_loss: %.7f, loss: %.7f, Lp: %.7f'%(ADD/cc,l_ov/cc,loss.cpu().detach().numpy(),torch.min(L_p).cpu().detach().numpy()))\n",
    "        \n",
    "    decay_stepper_r.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819f0424-bf31-42e1-a1ad-8b538b67c3f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
